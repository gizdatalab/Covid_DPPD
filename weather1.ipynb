{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Weather Data for DPPD Covid-19\n",
    "In this notebook we will download temperature, precipitation and sunshine duration data from _Deutscher Wetterdienst_ in order to include it into the Covid-19 Positive Deviance Analysis.\n",
    "\n",
    "Unfortunately, DWD does not provide access via an API, so we'll have to download the measurement data from each weather station for the last 500 days until yesterday and extract the desired information manually.\n",
    "\n",
    "Exact specifications of the data sets that are being used can be found here:\n",
    "- [temperature, hourly averages in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/BESCHREIBUNG_obsgermany_climate_hourly_tu_recent_de.pdf)\n",
    "- [precipitation, hourly averages in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/BESCHREIBUNG_obsgermany_climate_hourly_precipitation_recent_de.pdf)\n",
    "- [sunshine duration, hourly in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/BESCHREIBUNG_obsgermany_climate_hourly_sun_recent_de.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# file handling\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base urls to the server folders containing the data zip files\n",
    "url_temp = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/\"\n",
    "url_prec = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/\"\n",
    "url_sun = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/\"\n",
    "url_wind = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/\"\n",
    "\n",
    "# lists of the stations involved in measuring the corresponding data\n",
    "url_temp_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/TU_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_prec_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/RR_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_sun_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/SD_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_wind_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/FF_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# create folders to place downloaded content in\n",
    "path_base = Path.cwd()\n",
    "\n",
    "path_downloads_temp = Path.joinpath(path_base, \"downloads\", \"temp\")\n",
    "path_downloads_temp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_prec = Path.joinpath(path_base, \"downloads\", \"prec\")\n",
    "path_downloads_prec.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_sun = Path.joinpath(path_base, \"downloads\", \"sun\")\n",
    "path_downloads_sun.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_wind = Path.joinpath(path_base, \"downloads\", \"wind\")\n",
    "path_downloads_wind.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# folders for exported content\n",
    "path_export = Path.joinpath(path_base, \"exports\")\n",
    "path_export.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stations_description_parser(url):\n",
    "    \"\"\"Reads the stations description that lists all weather stations that participated in\n",
    "       measuring the corresponding climate property.\n",
    "       There exist around 700 stations that measure temperature and sun, and about 1000\n",
    "       stations that measure precipitation.\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    text = req.text.splitlines()\n",
    "\n",
    "    data = []\n",
    "    for line in text[2:]:\n",
    "        e = line.split()\n",
    "        station_id = e.pop(0)\n",
    "        start_date = e.pop(0)\n",
    "        end_date = e.pop(0)\n",
    "        altitude = e.pop(0)\n",
    "        latitude = e.pop(0)\n",
    "        longitude = e.pop(0)\n",
    "        state = e.pop(-1)\n",
    "        station_name = \" \".join(e)\n",
    "\n",
    "        row = [station_id, start_date, end_date, altitude, latitude, longitude, station_name, state]\n",
    "        data.append(row)\n",
    "\n",
    "    columns = [\"station_id\", \"start_date\", \"end_date\", \"altitude\", \"latitude\", \"longitude\", \"name\", \"state\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # convert columns to numeric \n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    \n",
    "    # convert dates into datetime objects\n",
    "    df[[\"start_date\", \"end_date\"]] = df[[\"start_date\", \"end_date\"]].apply(pd.to_datetime, format=\"%Y%m%d\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_file_urls(url, prefix, suffix):\n",
    "    \"\"\"Find all zip files that are available on the DWD server.\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "    anchors = soup.find_all(\"a\")\n",
    "    links = []\n",
    "\n",
    "    for a in anchors:\n",
    "        ref = a.get(\"href\")\n",
    "        if ref.startswith(prefix) and ref.endswith(suffix):\n",
    "            links.append(ref)\n",
    "    return links\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def download_zips(filenames, url_server, path_destination):\n",
    "    \"\"\"Downloads all zip files from the corresponding DWD server directory, extracts them and\n",
    "       keeps only the zip file content.\n",
    "    \"\"\"\n",
    "\n",
    "    if not Path(path_destination).is_dir():\n",
    "        print(\"Invalid directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading files from {url_server} to {path_destination}...\")\n",
    "    time.sleep(0.5) # otherwise progress bar gets messed up\n",
    "    for file in tqdm(filenames):\n",
    "        req = requests.get(url_server + file)\n",
    "\n",
    "        filename = Path.joinpath(path_destination, file)\n",
    "        filename.write_bytes(req.content)\n",
    "\n",
    "        # unzip the file and only keep the extracted content\n",
    "        with ZipFile(filename, \"r\") as zippy:\n",
    "            dirname = Path.joinpath(path_destination, file[:-4])\n",
    "            \n",
    "            # remove already-existing directory from previous run of this notebook\n",
    "            if os.path.isdir(dirname):\n",
    "                try:\n",
    "                    shutil.rmtree(dirname)\n",
    "                except OSError as e:\n",
    "                    print(e)\n",
    "                    print(\"Old downloads could not be removed by the program. \" +\n",
    "                          \"This might happen if you are working via SSH. \" + \n",
    "                          \"Try removing the corresponding folders manually.\")\n",
    "\n",
    "            try:\n",
    "                dirname.mkdir()\n",
    "                zippy.extractall(dirname)\n",
    "#                 os.remove(filename)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "def scan_downloaded_folders(directory, prefix, suffix):\n",
    "    \"\"\"Searches the specified directory for folders that match the prefix and the suffix\n",
    "       and returns a list of paths of all these folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_list = []\n",
    "\n",
    "    dir_iter = os.scandir(directory)\n",
    "    for i in dir_iter:\n",
    "        if i.is_dir():\n",
    "            name = i.name\n",
    "            if name.startswith(prefix) and name.endswith(suffix):\n",
    "                dir_list.append(Path.joinpath(directory, name))\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_filenames = scrape_file_urls(url_temp, \"stundenwerte_TU_\", \"_akt.zip\")\n",
    "prec_filenames = scrape_file_urls(url_prec, \"stundenwerte_RR_\", \"_akt.zip\")\n",
    "sun_filenames = scrape_file_urls(url_sun, \"stundenwerte_SD_\", \"_akt.zip\")\n",
    "wind_filenames = scrape_file_urls(url_wind, \"stundenwerte_FF_\", \"_akt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to download 1GB? (y/n)y\n",
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\analysis_for_Azure\\downloads\\temp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 512/512 [01:17<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\analysis_for_Azure\\downloads\\prec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 972/972 [02:26<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\analysis_for_Azure\\downloads\\sun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 303/303 [00:44<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\analysis_for_Azure\\downloads\\wind...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 294/294 [00:47<00:00,  6.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# download all the zip files (about 960 MB)\n",
    "answer = None\n",
    "answer = input(\"Do you want to download 1GB? (y/n)\")\n",
    "if answer == \"y\":\n",
    "    download_zips(temp_filenames, url_temp, path_downloads_temp)\n",
    "    download_zips(prec_filenames, url_prec, path_downloads_prec)\n",
    "    download_zips(sun_filenames, url_sun, path_downloads_sun)\n",
    "    download_zips(wind_filenames, url_wind, path_downloads_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find downloaded folders\n",
    "downloaded_temp_folders = scan_downloaded_folders(path_downloads_temp, \"stundenwerte_TU_\", \"_akt\")\n",
    "downloaded_prec_folders = scan_downloaded_folders(path_downloads_prec, \"stundenwerte_RR_\", \"_akt\")\n",
    "downloaded_sun_folders = scan_downloaded_folders(path_downloads_sun, \"stundenwerte_SD_\", \"_akt\")\n",
    "downloaded_wind_folders = scan_downloaded_folders(path_downloads_wind, \"stundenwerte_FF_\", \"_akt\")\n",
    "# downloaded_wind_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded_wind_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_product_file(directory, prefix, suffix):\n",
    "    \"\"\"Searches the extracted content of a downloaded zip file for the product file,\n",
    "       which contains the information we are interested in. There exist additional files\n",
    "       that provide meta data which we will ignore for now.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    \n",
    "    dir_iter = os.scandir(directory)\n",
    "    for i in dir_iter:\n",
    "        if i.is_file():\n",
    "            name = i.name\n",
    "            if name.startswith(prefix) and name.endswith(suffix):\n",
    "                file_list.append(Path.joinpath(directory, name))\n",
    "    \n",
    "    # make sure that only one product file was contained in the folder\n",
    "    if len(file_list) == 1:\n",
    "        return file_list[0]\n",
    "    elif len(file_list) > 1:\n",
    "        raise Exception(\"There seem to exist two product files for the same station!\")\n",
    "    else:\n",
    "        raise Exception(\"No product file found!\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def temp_to_dataframe(file_path):\n",
    "    \"\"\"Converts the downloaded temperature text file into a pandas dataframe.\n",
    "    \"\"\"\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"temperature\", \"humidity\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prec_to_dataframe(file_path):\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"R1\", \"R1_IND\", \"WRTR\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def sun_to_dataframe(file_path):\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"SD_SO\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "def wind_to_dataframe(file_path):\n",
    "    \"\"\"Converts the downloaded wind text file into a pandas dataframe.\n",
    "    \"\"\"\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"velocity\", \"direction\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "def collect_all_product_files(dirlist, prefix, suffix):\n",
    "    frames = []\n",
    "    for folder_name in dirlist:\n",
    "        #path = Path.joinpath(rootdir, folder_name)\n",
    "        path = Path(folder_name)\n",
    "        # find product file name\n",
    "        try:\n",
    "            product_file = find_product_file(path, prefix, suffix)\n",
    "        except:\n",
    "            print(\"Product file not found\")\n",
    "            continue\n",
    "\n",
    "        # extract the data from this file\n",
    "        if prefix == \"produkt_tu_stunde_\":\n",
    "            df = temp_to_dataframe(product_file)\n",
    "            frames.append(df)\n",
    "        elif prefix == \"produkt_rr_stunde_\":\n",
    "            df = prec_to_dataframe(product_file)\n",
    "        elif prefix == \"produkt_sd_stunde_\":\n",
    "            df = sun_to_dataframe(product_file)\n",
    "        elif prefix == \"produkt_ff_stunde_\":\n",
    "            df = wind_to_dataframe(product_file)\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported prefix {prefix}!\")\n",
    "            \n",
    "        frames.append(df)\n",
    "\n",
    "    # concatenate all the frames into one frame\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data\n",
    "df_temp = collect_all_product_files(downloaded_temp_folders, \"produkt_tu_stunde_\", \".txt\")\n",
    "df_prec = collect_all_product_files(downloaded_prec_folders, \"produkt_rr_stunde_\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sun = collect_all_product_files(downloaded_sun_folders, \"produkt_sd_stunde_\", \".txt\")\n",
    "df_wind = collect_all_product_files(downloaded_wind_folders, \"produkt_ff_stunde_\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_stations = stations_description_parser(url_temp_stations)\n",
    "df_prec_stations = stations_description_parser(url_prec_stations)\n",
    "df_sun_stations = stations_description_parser(url_sun_stations)\n",
    "df_wind_stations = stations_description_parser(url_wind_stations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d%H\")\n",
    "\n",
    "    # -999 = missing data\n",
    "    if 'temperature' in df.columns:\n",
    "        df.loc[df[\"temperature\"] == -999, \"temperature\"] = float(\"NaN\")\n",
    "    if 'humidity' in df.columns:\n",
    "        df.loc[df[\"humidity\"] == -999, \"humidity\"] = float(\"NaN\")\n",
    "    if 'R1' in df.columns:\n",
    "        df.loc[df[\"R1\"] == -999, \"R1\"] = float(\"NaN\")\n",
    "    if 'R1_IND' in df.columns:\n",
    "        df.loc[df[\"R1_IND\"] == -999, \"R1_IND\"] = float(\"NaN\")\n",
    "    if 'SD_SO' in df.columns:\n",
    "        df.loc[df[\"SD_SO\"] == -999, \"SD_SO\"] = float(\"NaN\")\n",
    "    if 'velocity' in df.columns:\n",
    "        df.loc[df[\"velocity\"] == -999, \"velocity\"] = float(\"NaN\")\n",
    "    if 'direction' in df.columns:\n",
    "        df.loc[df[\"direction\"] == -999, \"direction\"] = float(\"NaN\")\n",
    "    return df\n",
    "\n",
    "df_temp = clean_data(df_temp)\n",
    "df_prec = clean_data(df_prec)\n",
    "df_sun = clean_data(df_sun)\n",
    "df_wind = clean_data(df_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>quality</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>6314</td>\n",
       "      <td>2020-03-22 15:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12911</th>\n",
       "      <td>4560</td>\n",
       "      <td>2020-09-25 23:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>8.5</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>6346</td>\n",
       "      <td>2019-10-05 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>10.1</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>3527</td>\n",
       "      <td>2019-11-18 01:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>4160</td>\n",
       "      <td>2019-10-28 19:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12988</th>\n",
       "      <td>817</td>\n",
       "      <td>2020-09-29 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11965</th>\n",
       "      <td>6159</td>\n",
       "      <td>2020-08-17 13:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>20.6</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13163</th>\n",
       "      <td>5930</td>\n",
       "      <td>2020-10-06 11:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9428</th>\n",
       "      <td>4393</td>\n",
       "      <td>2020-05-03 20:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>8.6</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8606</th>\n",
       "      <td>1863</td>\n",
       "      <td>2020-03-30 14:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2638</td>\n",
       "      <td>2019-05-01 19:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4662</th>\n",
       "      <td>4301</td>\n",
       "      <td>2019-10-18 06:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>14.5</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>1736</td>\n",
       "      <td>2019-06-18 16:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>28.5</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>4625</td>\n",
       "      <td>2020-07-07 11:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>17.4</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2211</td>\n",
       "      <td>2019-05-01 20:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>2559</td>\n",
       "      <td>2019-07-18 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>12.4</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>5705</td>\n",
       "      <td>2019-07-21 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>26.6</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>4896</td>\n",
       "      <td>2019-04-14 22:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11029</th>\n",
       "      <td>282</td>\n",
       "      <td>2020-07-09 13:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>26.6</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156</th>\n",
       "      <td>2708</td>\n",
       "      <td>2020-04-22 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_id                date  quality  temperature  humidity\n",
       "8415         6314 2020-03-22 15:00:00        3          1.8      47.0\n",
       "12911        4560 2020-09-25 23:00:00        1          8.5      94.0\n",
       "4347         6346 2019-10-05 03:00:00        3         10.1      86.0\n",
       "5401         3527 2019-11-18 01:00:00        3          3.3     100.0\n",
       "4915         4160 2019-10-28 19:00:00        3          7.1      87.0\n",
       "12988         817 2020-09-29 04:00:00        1          6.5      96.0\n",
       "11965        6159 2020-08-17 13:00:00        1         20.6      90.0\n",
       "13163        5930 2020-10-06 11:00:00        1         13.0      95.0\n",
       "9428         4393 2020-05-03 20:00:00        3          8.6      75.0\n",
       "8606         1863 2020-03-30 14:00:00        3          7.1      30.0\n",
       "595          2638 2019-05-01 19:00:00        3         10.8      55.0\n",
       "4662         4301 2019-10-18 06:00:00        3         14.5      90.0\n",
       "1744         1736 2019-06-18 16:00:00        3         28.5      35.0\n",
       "10979        4625 2020-07-07 11:00:00        3         17.4      50.0\n",
       "596          2211 2019-05-01 20:00:00        3         10.8      81.0\n",
       "2448         2559 2019-07-18 00:00:00        3         12.4      92.0\n",
       "2532         5705 2019-07-21 12:00:00        3         26.6      44.0\n",
       "190          4896 2019-04-14 22:00:00        3          4.9      74.0\n",
       "11029         282 2020-07-09 13:00:00        3         26.6      48.0\n",
       "9156         2708 2020-04-22 12:00:00        3         17.2      35.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station_id             int64\n",
       "date          datetime64[ns]\n",
       "quality                int64\n",
       "velocity             float64\n",
       "direction            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wind.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard all data from before 2020\n",
    "df_temp = df_temp[df_temp['date'].dt.year == 2020]\n",
    "df_prec = df_prec[df_prec['date'].dt.year == 2020]\n",
    "df_sun = df_sun[df_sun['date'].dt.year == 2020]\n",
    "df_wind = df_wind[df_wind['date'].dt.year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as pickle\n",
    "df_temp.to_pickle(Path.joinpath(path_export, \"temp.pkl\"))\n",
    "df_temp_stations.to_pickle(Path.joinpath(path_export, \"temp_stations.pkl\"))\n",
    "\n",
    "df_prec.to_pickle(Path.joinpath(path_export, \"prec.pkl\"))\n",
    "df_prec_stations.to_pickle(Path.joinpath(path_export, \"prec_stations.pkl\"))\n",
    "\n",
    "df_sun.to_pickle(Path.joinpath(path_export, \"sun.pkl\"))\n",
    "df_prec_stations.to_pickle(Path.joinpath(path_export, \"sun_stations.pkl\"))\n",
    "\n",
    "df_wind.to_pickle(Path.joinpath(path_export, \"wind.pkl\"))\n",
    "df_wind_stations.to_pickle(Path.joinpath(path_export, \"wind_stations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
