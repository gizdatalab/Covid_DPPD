{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Weather Data for DPPD Covid-19\n",
    "In this notebook we will download temperature, precipitation and sunshine duration data from _Deutscher Wetterdienst_ in order to include it into the Covid-19 Positive Deviance Analysis.\n",
    "\n",
    "Unfortunately, DWD does not provide access via an API, so we'll have to download the measurement data from each weather station for the last 500 days until yesterday and extract the desired information manually.\n",
    "\n",
    "Exact specifications of the data sets that are being used can be found here:\n",
    "- [temperature, hourly averages in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/BESCHREIBUNG_obsgermany_climate_hourly_tu_recent_de.pdf)\n",
    "- [precipitation, hourly averages in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/BESCHREIBUNG_obsgermany_climate_hourly_precipitation_recent_de.pdf)\n",
    "- [sunshine duration, hourly in Germany](https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/BESCHREIBUNG_obsgermany_climate_hourly_sun_recent_de.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# file handling\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base urls to the server folders containing the data zip files\n",
    "url_temp = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/\"\n",
    "url_prec = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/\"\n",
    "url_sun = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/\"\n",
    "url_wind = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/\"\n",
    "\n",
    "# lists of the stations involved in measuring the corresponding data\n",
    "url_temp_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/TU_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_prec_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/RR_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_sun_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/SD_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "url_wind_stations = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/FF_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# create folders to place downloaded content in\n",
    "path_base = Path.cwd()\n",
    "\n",
    "path_downloads_temp = Path.joinpath(path_base, \"downloads\", \"temp\")\n",
    "path_downloads_temp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_prec = Path.joinpath(path_base, \"downloads\", \"prec\")\n",
    "path_downloads_prec.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_sun = Path.joinpath(path_base, \"downloads\", \"sun\")\n",
    "path_downloads_sun.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "path_downloads_wind = Path.joinpath(path_base, \"downloads\", \"wind\")\n",
    "path_downloads_wind.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# folders for exported content\n",
    "path_export = Path.joinpath(path_base, \"exports\")\n",
    "path_export.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stations_description_parser(url):\n",
    "    \"\"\"Reads the stations description that lists all weather stations that participated in\n",
    "       measuring the corresponding climate property.\n",
    "       There exist around 700 stations that measure temperature and sun, and about 1000\n",
    "       stations that measure precipitation.\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    text = req.text.splitlines()\n",
    "\n",
    "    data = []\n",
    "    for line in text[2:]:\n",
    "        e = line.split()\n",
    "        station_id = e.pop(0)\n",
    "        start_date = e.pop(0)\n",
    "        end_date = e.pop(0)\n",
    "        altitude = e.pop(0)\n",
    "        latitude = e.pop(0)\n",
    "        longitude = e.pop(0)\n",
    "        state = e.pop(-1)\n",
    "        station_name = \" \".join(e)\n",
    "\n",
    "        row = [station_id, start_date, end_date, altitude, latitude, longitude, station_name, state]\n",
    "        data.append(row)\n",
    "\n",
    "    columns = [\"station_id\", \"start_date\", \"end_date\", \"altitude\", \"latitude\", \"longitude\", \"name\", \"state\"]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # convert columns to numeric \n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    \n",
    "    # convert dates into datetime objects\n",
    "    df[[\"start_date\", \"end_date\"]] = df[[\"start_date\", \"end_date\"]].apply(pd.to_datetime, format=\"%Y%m%d\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_file_urls(url, prefix, suffix):\n",
    "    \"\"\"Find all zip files that are available on the DWD server.\n",
    "    \"\"\"\n",
    "    req = requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "    anchors = soup.find_all(\"a\")\n",
    "    links = []\n",
    "\n",
    "    for a in anchors:\n",
    "        ref = a.get(\"href\")\n",
    "        if ref.startswith(prefix) and ref.endswith(suffix):\n",
    "            links.append(ref)\n",
    "    return links\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def download_zips(filenames, url_server, path_destination):\n",
    "    \"\"\"Downloads all zip files from the corresponding DWD server directory, extracts them and\n",
    "       keeps only the zip file content.\n",
    "    \"\"\"\n",
    "\n",
    "    if not Path(path_destination).is_dir():\n",
    "        print(\"Invalid directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading files from {url_server} to {path_destination}...\")\n",
    "    time.sleep(0.5) # otherwise progress bar gets messed up\n",
    "    for file in tqdm(filenames):\n",
    "        req = requests.get(url_server + file)\n",
    "\n",
    "        filename = Path.joinpath(path_destination, file)\n",
    "        filename.write_bytes(req.content)\n",
    "\n",
    "        # unzip the file and only keep the extracted content\n",
    "        with ZipFile(filename, \"r\") as zippy:\n",
    "            dirname = Path.joinpath(path_destination, file[:-4])\n",
    "            \n",
    "            # remove already-existing directory from previous run of this notebook\n",
    "            if os.path.isdir(dirname):\n",
    "                try:\n",
    "                    shutil.rmtree(dirname)\n",
    "                except OSError as e:\n",
    "                    print(e)\n",
    "                    print(\"Old downloads could not be removed by the program. \" +\n",
    "                          \"This might happen if you are working via SSH. \" + \n",
    "                          \"Try removing the corresponding folders manually.\")\n",
    "\n",
    "            try:\n",
    "                dirname.mkdir()\n",
    "                zippy.extractall(dirname)\n",
    "#                 os.remove(filename)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "def scan_downloaded_folders(directory, prefix, suffix):\n",
    "    \"\"\"Searches the specified directory for folders that match the prefix and the suffix\n",
    "       and returns a list of paths of all these folders.\n",
    "    \"\"\"\n",
    "    \n",
    "    dir_list = []\n",
    "\n",
    "    dir_iter = os.scandir(directory)\n",
    "    for i in dir_iter:\n",
    "        if i.is_dir():\n",
    "            name = i.name\n",
    "            if name.startswith(prefix) and name.endswith(suffix):\n",
    "                dir_list.append(Path.joinpath(directory, name))\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_filenames = scrape_file_urls(url_temp, \"stundenwerte_TU_\", \"_akt.zip\")\n",
    "prec_filenames = scrape_file_urls(url_prec, \"stundenwerte_RR_\", \"_akt.zip\")\n",
    "sun_filenames = scrape_file_urls(url_sun, \"stundenwerte_SD_\", \"_akt.zip\")\n",
    "wind_filenames = scrape_file_urls(url_wind, \"stundenwerte_FF_\", \"_akt.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to download 1GB? (y/n)y\n",
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\siemens_hackathon_stuff\\weather_4_hackathon\\downloads\\temp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 512/512 [01:53<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\siemens_hackathon_stuff\\weather_4_hackathon\\downloads\\prec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 972/972 [03:19<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/sun/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\siemens_hackathon_stuff\\weather_4_hackathon\\downloads\\sun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 303/303 [01:01<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files from https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/wind/recent/ to C:\\Users\\joshu\\OneDrive\\Desktop\\giz_dppd_covid19\\siemens_hackathon_stuff\\weather_4_hackathon\\downloads\\wind...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 294/294 [01:08<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# download all the zip files (about 960 MB)\n",
    "answer = None\n",
    "answer = input(\"Do you want to download 1GB? (y/n)\")\n",
    "if answer == \"y\":\n",
    "    download_zips(temp_filenames, url_temp, path_downloads_temp)\n",
    "    download_zips(prec_filenames, url_prec, path_downloads_prec)\n",
    "    download_zips(sun_filenames, url_sun, path_downloads_sun)\n",
    "    download_zips(wind_filenames, url_wind, path_downloads_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find downloaded folders\n",
    "downloaded_temp_folders = scan_downloaded_folders(path_downloads_temp, \"stundenwerte_TU_\", \"_akt\")\n",
    "downloaded_prec_folders = scan_downloaded_folders(path_downloads_prec, \"stundenwerte_RR_\", \"_akt\")\n",
    "downloaded_sun_folders = scan_downloaded_folders(path_downloads_sun, \"stundenwerte_SD_\", \"_akt\")\n",
    "downloaded_wind_folders = scan_downloaded_folders(path_downloads_wind, \"stundenwerte_FF_\", \"_akt\")\n",
    "# downloaded_wind_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded_wind_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_product_file(directory, prefix, suffix):\n",
    "    \"\"\"Searches the extracted content of a downloaded zip file for the product file,\n",
    "       which contains the information we are interested in. There exist additional files\n",
    "       that provide meta data which we will ignore for now.\n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    \n",
    "    dir_iter = os.scandir(directory)\n",
    "    for i in dir_iter:\n",
    "        if i.is_file():\n",
    "            name = i.name\n",
    "            if name.startswith(prefix) and name.endswith(suffix):\n",
    "                file_list.append(Path.joinpath(directory, name))\n",
    "    \n",
    "    # make sure that only one product file was contained in the folder\n",
    "    if len(file_list) == 1:\n",
    "        return file_list[0]\n",
    "    elif len(file_list) > 1:\n",
    "        raise Exception(\"There seem to exist two product files for the same station!\")\n",
    "    else:\n",
    "        raise Exception(\"No product file found!\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def temp_to_dataframe(file_path):\n",
    "    \"\"\"Converts the downloaded temperature text file into a pandas dataframe.\n",
    "    \"\"\"\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"temperature\", \"humidity\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def prec_to_dataframe(file_path):\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"R1\", \"R1_IND\", \"WRTR\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def sun_to_dataframe(file_path):\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"SD_SO\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "def wind_to_dataframe(file_path):\n",
    "    \"\"\"Converts the downloaded wind text file into a pandas dataframe.\n",
    "    \"\"\"\n",
    "    text = file_path.read_text()\n",
    "    text = text.splitlines()\n",
    "    \n",
    "    data = []\n",
    "    for line in text[1:]:\n",
    "        row = line.split(\";\")\n",
    "        values = [value.strip() for value in row]\n",
    "        values.pop()\n",
    "        data.append(values)\n",
    "\n",
    "    colnames = [\"station_id\", \"date\", \"quality\", \"velocity\", \"direction\"]\n",
    "    df = pd.DataFrame(data, columns=colnames)\n",
    "\n",
    "    # TODO apply processing to columns (e.g. datetime)\n",
    "\n",
    "    return df\n",
    "\n",
    "def collect_all_product_files(dirlist, prefix, suffix):\n",
    "    frames = []\n",
    "    for folder_name in dirlist:\n",
    "        #path = Path.joinpath(rootdir, folder_name)\n",
    "        path = Path(folder_name)\n",
    "        # find product file name\n",
    "        try:\n",
    "            product_file = find_product_file(path, prefix, suffix)\n",
    "        except:\n",
    "            print(\"Product file not found\")\n",
    "            continue\n",
    "\n",
    "        # extract the data from this file\n",
    "        if prefix == \"produkt_tu_stunde_\":\n",
    "            df = temp_to_dataframe(product_file)\n",
    "            frames.append(df)\n",
    "        elif prefix == \"produkt_rr_stunde_\":\n",
    "            df = prec_to_dataframe(product_file)\n",
    "        elif prefix == \"produkt_sd_stunde_\":\n",
    "            df = sun_to_dataframe(product_file)\n",
    "        elif prefix == \"produkt_ff_stunde_\":\n",
    "            df = wind_to_dataframe(product_file)\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported prefix {prefix}!\")\n",
    "            \n",
    "        frames.append(df)\n",
    "\n",
    "    # concatenate all the frames into one frame\n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all data\n",
    "df_temp = collect_all_product_files(downloaded_temp_folders, \"produkt_tu_stunde_\", \".txt\")\n",
    "df_prec = collect_all_product_files(downloaded_prec_folders, \"produkt_rr_stunde_\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sun = collect_all_product_files(downloaded_sun_folders, \"produkt_sd_stunde_\", \".txt\")\n",
    "df_wind = collect_all_product_files(downloaded_wind_folders, \"produkt_ff_stunde_\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_stations = stations_description_parser(url_temp_stations)\n",
    "df_prec_stations = stations_description_parser(url_prec_stations)\n",
    "df_sun_stations = stations_description_parser(url_sun_stations)\n",
    "df_wind_stations = stations_description_parser(url_wind_stations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y%m%d%H\")\n",
    "\n",
    "    # -999 = missing data\n",
    "    if 'temperature' in df.columns:\n",
    "        df.loc[df[\"temperature\"] == -999, \"temperature\"] = float(\"NaN\")\n",
    "    if 'humidity' in df.columns:\n",
    "        df.loc[df[\"humidity\"] == -999, \"humidity\"] = float(\"NaN\")\n",
    "    if 'R1' in df.columns:\n",
    "        df.loc[df[\"R1\"] == -999, \"R1\"] = float(\"NaN\")\n",
    "    if 'R1_IND' in df.columns:\n",
    "        df.loc[df[\"R1_IND\"] == -999, \"R1_IND\"] = float(\"NaN\")\n",
    "    if 'SD_SO' in df.columns:\n",
    "        df.loc[df[\"SD_SO\"] == -999, \"SD_SO\"] = float(\"NaN\")\n",
    "    if 'velocity' in df.columns:\n",
    "        df.loc[df[\"velocity\"] == -999, \"velocity\"] = float(\"NaN\")\n",
    "    if 'direction' in df.columns:\n",
    "        df.loc[df[\"direction\"] == -999, \"direction\"] = float(\"NaN\")\n",
    "    return df\n",
    "\n",
    "df_temp = clean_data(df_temp)\n",
    "df_prec = clean_data(df_prec)\n",
    "df_sun = clean_data(df_sun)\n",
    "df_wind = clean_data(df_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>date</th>\n",
       "      <th>quality</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6616</th>\n",
       "      <td>5871</td>\n",
       "      <td>2020-01-04 16:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>5014</td>\n",
       "      <td>2019-09-27 04:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>2201</td>\n",
       "      <td>2019-10-06 21:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11714</th>\n",
       "      <td>4480</td>\n",
       "      <td>2020-08-04 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>10.3</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>4371</td>\n",
       "      <td>2020-02-04 21:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>102</td>\n",
       "      <td>2020-02-20 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>8.1</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6612</th>\n",
       "      <td>2578</td>\n",
       "      <td>2020-01-04 12:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>954</td>\n",
       "      <td>2020-06-09 01:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>11.5</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>3362</td>\n",
       "      <td>2019-08-26 21:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>18.8</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>3679</td>\n",
       "      <td>2019-11-16 15:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12083</th>\n",
       "      <td>5014</td>\n",
       "      <td>2020-08-19 11:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>24.1</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>4508</td>\n",
       "      <td>2019-10-03 16:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>1886</td>\n",
       "      <td>2019-11-20 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11656</th>\n",
       "      <td>1107</td>\n",
       "      <td>2020-08-01 16:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>31.2</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4472</th>\n",
       "      <td>6158</td>\n",
       "      <td>2019-10-07 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>13675</td>\n",
       "      <td>2019-08-08 14:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>25.2</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12714</th>\n",
       "      <td>1443</td>\n",
       "      <td>2020-09-14 18:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>22.9</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5648</th>\n",
       "      <td>13713</td>\n",
       "      <td>2019-11-25 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>3540</td>\n",
       "      <td>2020-06-29 10:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>7427</td>\n",
       "      <td>2019-09-03 16:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>18.7</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       station_id                date  quality  temperature  humidity\n",
       "6616         5871 2020-01-04 16:00:00        3          2.3      91.0\n",
       "4228         5014 2019-09-27 04:00:00        3         14.1      96.0\n",
       "4461         2201 2019-10-06 21:00:00        3          7.4      87.0\n",
       "11714        4480 2020-08-04 02:00:00        1         10.3      95.0\n",
       "7365         4371 2020-02-04 21:00:00        3          3.7      89.0\n",
       "7722          102 2020-02-20 12:00:00        3          8.1      90.0\n",
       "6612         2578 2020-01-04 12:00:00        3          5.4      83.0\n",
       "3706          954 2020-06-09 01:00:00        3         11.5      67.0\n",
       "3477         3362 2019-08-26 21:00:00        3         18.8      89.0\n",
       "5439         3679 2019-11-16 15:00:00        3          5.0      86.0\n",
       "12083        5014 2020-08-19 11:00:00        1         24.1      51.0\n",
       "4384         4508 2019-10-03 16:00:00        3          8.0      89.0\n",
       "5528         1886 2019-11-20 08:00:00        3          2.6      94.0\n",
       "11656        1107 2020-08-01 16:00:00        1         31.2      30.0\n",
       "4472         6158 2019-10-07 08:00:00        3          6.9      96.0\n",
       "1394        13675 2019-08-08 14:00:00        3         25.2      39.0\n",
       "12714        1443 2020-09-14 18:00:00        1         22.9      52.0\n",
       "5648        13713 2019-11-25 08:00:00        3          4.6      99.0\n",
       "10858        3540 2020-06-29 10:00:00        3         16.6      63.0\n",
       "3664         7427 2019-09-03 16:00:00        3         18.7      76.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard all data from before 2020\n",
    "df_temp = df_temp[df_temp['date'].dt.year == 2020]\n",
    "df_prec = df_prec[df_prec['date'].dt.year == 2020]\n",
    "df_sun = df_sun[df_sun['date'].dt.year == 2020]\n",
    "df_wind = df_wind[df_wind['date'].dt.year == 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as pickle\n",
    "df_temp.to_pickle(Path.joinpath(path_export, \"temp.pkl\"))\n",
    "df_temp_stations.to_pickle(Path.joinpath(path_export, \"temp_stations.pkl\"))\n",
    "\n",
    "df_prec.to_pickle(Path.joinpath(path_export, \"prec.pkl\"))\n",
    "df_prec_stations.to_pickle(Path.joinpath(path_export, \"prec_stations.pkl\"))\n",
    "\n",
    "df_sun.to_pickle(Path.joinpath(path_export, \"sun.pkl\"))\n",
    "df_prec_stations.to_pickle(Path.joinpath(path_export, \"sun_stations.pkl\"))\n",
    "\n",
    "df_wind.to_pickle(Path.joinpath(path_export, \"wind.pkl\"))\n",
    "df_wind_stations.to_pickle(Path.joinpath(path_export, \"wind_stations.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(df_wind, df_wind_stations, on='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
