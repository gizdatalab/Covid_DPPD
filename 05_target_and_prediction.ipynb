{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rki_weekly = pd.read_csv('rki_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rki_weekly['infs_last_week'] = rki_weekly.groupby('districtId')['AnzahlFall'].shift(1)\n",
    "rki_weekly['R0_last_week'] = (rki_weekly['AnzahlFall'] + 1) / (rki_weekly['infs_last_week'] + 1)\n",
    "rki_weekly['R0_this_week'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(-1)\n",
    "rki_weekly['target'] = np.log(rki_weekly['R0_this_week'])\n",
    "rki_weekly['auto_regr'] = np.log(rki_weekly['R0_last_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('processed_static_data.csv')\n",
    "mobility_features = pd.read_csv('mobility_features.csv')\n",
    "internationality  = pd.read_csv('internationality.csv')\n",
    "all_data = rki_weekly.merge(static_data).merge(mobility_features).merge(internationality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xpred import cross_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_variables = ['auto_regr',\n",
    "                       'population_density',\n",
    "                       'hospital_capacity',\n",
    "                       'share_of_nursing_home_care', #this one to be decided based on predictive power\n",
    "                       'average_age', \n",
    "                       'average_household_size',\n",
    "                       'share_female',\n",
    "                       'Count_internal_workday_p_pop',\n",
    "                       'Count_internal_day_off_p_pop',\n",
    "                       'Count_incoming_workday_p_pop', \n",
    "                       'Count_incoming_day_off_p_pop',\n",
    "                       'incoming_infected_p_pop',\n",
    "                       'log_intl_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_predict(data, how, model, X_cols, y_col, id_col = None, shuffle_mode = 'id', mod_params = None, mod_attrs_2_return = None, z_trim = None, random_state = 1):\n",
    "    \n",
    "    '''Function to cross-predict target variable across multiple groups.\n",
    "    Parameters:\n",
    "    data = Dataframe containing at least X_cols, y_col and the same (multi-)index as intended for the output, additional columns are dropped. Rows with missing values in any relevant column are dropped as well.\n",
    "    how = Either number of groups for cross-validation or string 's'/'student_resid' for studentized residuals, i.e. each observation as its own group, predicted by a model fitted on all data except the observation itself. Can be arbitrarily high, even higher than N of data, with resulting empty groups ignored. Setting how to 1 omits cross-prediction, and fits and uses the model on the entire or only z-trimmed data set instead.\n",
    "    model = String corresponding to the name of an already imported model class. Only classes with .fit() and .predict() methods will work.\n",
    "    X_col = List of column names used as features in target prediction\n",
    "    y_col = Column name of target variable\n",
    "    id_col = Single column name or list of column names needed for re-merging the results to other dataframes. If None, the dataframe index, passed through from the data input, can still be used\n",
    "    shuffle_mode = 'id': all values with the same ID are in the same group. 'full': every data point is shuffled independently, so that e.g. the same district can have weekly values in the training set and the prediction set.\n",
    "    mod_params = optional dictionary with named parameters for the model, e.g. max_depth for RF. The function's random state is passed as random_state parameter automatically.\n",
    "    mod_attrs_2_return = String name or list of names of attributes of the fitted model objects to return, e.g. 'coef_' for linear regression weights. Returns a dictionary with attribute name(s) as key(s) and corresponding list of attribute values as value(s).\n",
    "    z_trim = optional threshold for extreme outliers to be removed from training data, to avoid overfitting on these outliers. All observations with target values with a z-score above this value will be dropped from the training data. Does not affect test data, so that potential positive deviance can still be found without hurting the model.\n",
    "    random_state = seed for RandomState object used in all RNG processes for re-producible results.\n",
    "    \n",
    "    Returns:\n",
    "    output_df = Dataframe containing predicted target values, actual target values and grouping IDs from the cross-prediction process. Indexed with the same variables as the data input.\n",
    "    total_r2 = Determination coefficient of the cross-predictions compared to actual target values, across all groups.\n",
    "    train_accs = list of individual model performances per group for training set\n",
    "    test_accs = list of individual model performances per group for test set. Test sets with n = 1 will not be included here, since there is no variance to be explained. Test n = 1 results either from how = 'student_resid', or from there being so many groups that some or all of them are either empty or contain only one value.\n",
    "    mod_attrs = dictionary of lists with the requested model attributes to return, keyed with the attribute name.\n",
    "    \n",
    "    Note: Function is written for model objects with sklearn syntax, e.g. initializing without arguments, fitting in place.\n",
    "    Statsmodels uses different syntax, initializing with endogenous and exogenous variables, returning the fitted model. Instead of writing exception handling for all that, it's easier to just write an sklearn-esque version of the Statsmodels function if need be, like here for ordinary least squares regression:\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "    class sm_ols():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = sm.add_constant(X)\n",
    "        self.mod = sm.OLS(y,X).fit()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = sm.add_constant(X)\n",
    "        predictions = self.mod.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if id_col is not None:\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col,id_col]) #flatten, since X and id can \n",
    "    else:  #be either one string or a list of strings\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col])\n",
    "    df = data[all_cols].copy() #drop columns not needed\n",
    "    df.dropna(axis=0, inplace=True) #rowwise delete NAs \n",
    "    X = df[X_cols] #select predictors\n",
    "    y = df[y_col] #select target\n",
    "    n = df.shape[0] #get sample size (after NA removal)\n",
    "    rand = np.random.RandomState(random_state) #init random state\n",
    "    \n",
    "    train_accs = [] #init list of individual training accuracies\n",
    "    test_accs = [] #init list of individual test accuracies\n",
    "    if mod_attrs_2_return is not None: #init output for model attributes\n",
    "        if type(mod_attrs_2_return) == str: #if a single attribute name string,\n",
    "            mod_attrs = {mod_attrs_2_return:[]} #the function will output a list of these attribute values\n",
    "        else: #else, we'll assume an iterable input, where each attribute will be\n",
    "            mod_attrs = {} #used as the key in a dictionary of lists\n",
    "            for a in mod_attrs_2_return: #of attribute values that is initialised emptily\n",
    "                mod_attrs[a] = [] #here.\n",
    "    \n",
    "    output_df = pd.DataFrame() #init dataframe for output\n",
    "    pred_var_name = y_col + '_predicted' #define name for predicted target variable\n",
    "    \n",
    "    if shuffle_mode == 'full':#every row of the dataframe is treated independently\n",
    "        if how == 'student_resid' or how == 's': #if studentized residuals are wanted, \n",
    "            group = np.arange(n) #each observation is its own group\n",
    "        else:\n",
    "            try: #otherwise assign to specified number of groups\n",
    "                group = np.arange(n) % how\n",
    "                rand.shuffle(group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "            except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "    elif shuffle_mode == 'id': #all values of one id must be in the same group\n",
    "        if id_col is None:\n",
    "            raise ValueError('No ID cols to shuffle with!')\n",
    "        else:\n",
    "            unique_IDs = df.groupby(id_col).size().reset_index()[id_col] \n",
    "            #this gets all unique ID values for a single string, as well as all unique combinations for a list of strings\n",
    "            n_uniq = unique_IDs.shape[0] #number of unique ID(combination)s as length of group column to be created\n",
    "            \n",
    "            if how =='student_resid' or how == 's':\n",
    "                ID_group = np.arange(n_uniq) #each unique ID(combination) gets its own group\n",
    "            else:\n",
    "                try: #otherwise assign to specified number of groups\n",
    "                    ID_group = np.arange(n_uniq) % how\n",
    "                    rand.shuffle(ID_group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "                except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                    raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "            \n",
    "            unique_IDs['group'] = ID_group #add group to the dataframe of unique IDs so that\n",
    "            df_with_group = pd.merge(df, unique_IDs) #we can merge it with the dataframe to get the \n",
    "            group = np.array(df_with_group['group']) #array matching each row of the full dataset to its respective group\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('shuffle mode must be either \\'id\\' or \\'full\\'!')\n",
    "            \n",
    "    for g in np.unique(group):#iterate over groups\n",
    "        \n",
    "        if how == 1: #if only one group, \n",
    "            X_train, y_train = X, y #train on entire dataset.\n",
    "            X_test, y_test = X, y #this would happen anyway since all data is in group g=0, included here only as failsafe.\n",
    "        else:\n",
    "            X_train = X[group != g] #train model on all data NOT in the group\n",
    "            y_train = y[group != g]\n",
    "                \n",
    "            X_test = X[group == g] #test/predict in group data\n",
    "            y_test = y[group == g]\n",
    "\n",
    "            \n",
    "        if z_trim is not None: #if extreme values should be trimmed from training data\n",
    "            m_y = np.mean(y_train) #calc train target mean\n",
    "            s_y = np.std(y_train) #and SD\n",
    "            z_y = (y_train - m_y) / s_y #for z-values\n",
    "            X_train = X_train[abs(z_y) <= z_trim] #and cut all observations with target values\n",
    "            y_train = y_train[abs(z_y) <= z_trim] #more extreme than z_trim threshold\n",
    "        \n",
    "        #defining model by trying to pass string input as (previously imported) model class name.\n",
    "        #Both passing and not passing call brackets will work.\n",
    "        try: \n",
    "            mod = eval(model+'()')\n",
    "        except:\n",
    "            try:\n",
    "                mod = eval(model)\n",
    "            except:\n",
    "                raise ValueError('\\'model\\' parameter could not be interpreted as model class. Check whether you have imported the corresponding class.')\n",
    "\n",
    "        setattr(mod, 'random_state', rand)#if model uses RNG, passing the function's RandomState object here. Will be passively ignored by e.g. LinearRegression().\n",
    "\n",
    "        if mod_params is not None: #if model params are provided as a dict\n",
    "            for k, v in mod_params.items(): #iterate over dict and\n",
    "                setattr(mod,k,v) #set attributes of mod accordingly\n",
    "                #meaningless attributes are added without effect, e.g.\n",
    "                #max_depth for LinearRegression will be set without error\n",
    "                #because it is simply ignored\n",
    "        \n",
    "        mod.fit(X_train, y_train) #fit model to training data\n",
    "\n",
    "        from sklearn.metrics import r2_score\n",
    "\n",
    "        train_accs.append(r2_score(y_train, mod.predict(X_train))) #save model performance for training set\n",
    "        \n",
    "        y_pred = mod.predict(X_test) #predict target variable for data in current group\n",
    "        #this will be the prediction that 'counts' for this data, as it is the only prediction\n",
    "        #for this data where the model has not seen this data before\n",
    "        \n",
    "        if len(y_pred) > 1: #y_pred will be 1 when studentized residuals are chosen or when groups become too small\n",
    "            test_accs.append(r2_score(y_test, y_pred)) #if there are enough (2) observations, save test performance as well\n",
    "            \n",
    "        group_output = pd.DataFrame({pred_var_name:y_pred, #the predicted target\n",
    "                                    y_col:y_test, #values, the actual target values and the\n",
    "                                    'grouping_id':g}) #grouping IDs for potential trouble shooting\n",
    "        if id_col is None: #if no ID columns are supposed to be passed through,\n",
    "            pass #no action necessary\n",
    "        elif type(id_col) == str: #alternatively, if one col name string is given,\n",
    "            group_output[id_col] = df[id_col][group == g] #add this column to group output\n",
    "        else: #else, id_col is assumed to be iterable (a list). id_cols that are neither strings, nor iterables\n",
    "            for i in id_col: #will throw a non-iterable error. Here, we iterate over the list and\n",
    "                group_output[i] = df[i][group == g] #add each col to output\n",
    "        \n",
    "        output_df = pd.concat([output_df, group_output]) #add output for current group to overall output\n",
    "        \n",
    "        if mod_attrs_2_return is None: #if no mod attributes are wanted,\n",
    "            pass #no action is necessary\n",
    "        elif type(mod_attrs_2_return) == str: #if one attribute name string is given,\n",
    "            mod_attrs[mod_attrs_2_return].append(getattr(mod, mod_attrs_2_return)) #add the current models value to list\n",
    "        else: #else, input is assumed to be iterable (a list). If neither string, nor iterable, an error is raised about\n",
    "            for a in mod_attrs_2_return: #it not being iterable. Here, we iterate over the attributes and append\n",
    "                mod_attrs[a].append(getattr(mod, a)) #each attribute to the right list in the attribute dict\n",
    "        \n",
    "    output_df.sort_index(inplace=True)#revert order to index ordering, as it is now sorted by group first\n",
    "    #If the input dataframe was sorted by its index, this should result in \n",
    "    #the same order as the input dataframe, apart from the dropped rows due to missing values.\n",
    "    total_r2 = r2_score(output_df[y_col],output_df[pred_var_name]) #calc R2 for all data combined\n",
    "       \n",
    "    print('Overall, the cross-predictions accounted for {:.2%} of the target variance.'.format(total_r2))   \n",
    "                                                                                            \n",
    "    if mod_attrs_2_return is not None:\n",
    "        return output_df, total_r2, train_accs, test_accs, mod_attrs\n",
    "    else:\n",
    "        return output_df, total_r2, train_accs, test_accs, None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0, N = 93\n",
      "Overall, the cross-predictions accounted for 42.18% of the target variance.\n",
      "Cluster: 1, N = 210\n",
      "Overall, the cross-predictions accounted for 25.18% of the target variance.\n",
      "Cluster: 2, N = 318\n",
      "Overall, the cross-predictions accounted for 37.95% of the target variance.\n",
      "Cluster: 3, N = 189\n",
      "Overall, the cross-predictions accounted for 9.55% of the target variance.\n",
      "Cluster: 4, N = 390\n",
      "Overall, the cross-predictions accounted for 26.33% of the target variance.\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame()\n",
    "tr2 = []\n",
    "tr = []\n",
    "te = []\n",
    "mod_coefs = []\n",
    "for clust in np.unique(all_data.cluster):\n",
    "    clust_df = all_data[all_data.cluster == clust].copy()\n",
    "    clust_N = clust_df.shape[0]\n",
    "    print('Cluster: {}, N = {}'.format(clust,clust_N))\n",
    "        \n",
    "    clust_output, r2, tr_, te_, mod_attr = cross_predict(data = clust_df,\n",
    "                                                         X_cols = predictor_variables,\n",
    "                                                         y_col = 'target',\n",
    "                                                         id_col = ['districtId','district_name'],\n",
    "                                                         how = 's',\n",
    "                                                         model = 'LinearRegression',\n",
    "                                                         mod_attrs_2_return = 'coef_',\n",
    "                                                         z_trim = 3,\n",
    "                                                         mod_params=None)\n",
    "\n",
    "    predictions = pd.concat([predictions, clust_output])\n",
    "    tr2.append(r2)\n",
    "    tr.append(tr_)\n",
    "    te.append(te_)\n",
    "    mod_coefs.append(mod_attr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['deviance'] = predictions['target_predicted'] - predictions['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('deviances.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
