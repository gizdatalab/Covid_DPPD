{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load processed weekly RKI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rki_weekly = pd.read_csv('rki_weekly.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create weekly reproduction factors and shifted and logarithmized versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_as_default = True #False would turn div-by-zero and zero R0s into NaNs\n",
    "#model performance decreases when False, but distribution of target has less of\n",
    "#a spike at 0. Still, having gap-less panel data is better.\n",
    "\n",
    "rki_weekly['infs_last_week'] = rki_weekly.groupby('districtId')['AnzahlFall'].shift(1)\n",
    "\n",
    "if one_as_default:\n",
    "    rki_weekly['R0_last_week'] = (rki_weekly['AnzahlFall'] + 1) / (rki_weekly['infs_last_week'] + 1)\n",
    "else:\n",
    "    rki_weekly['R0_last_week'] = (rki_weekly['AnzahlFall']) / (rki_weekly['infs_last_week'])\n",
    "    rki_weekly['R0_last_week'].replace([np.inf, 0],np.nan,inplace=True)\n",
    "\n",
    "rki_weekly['R0_this_week'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(-1)\n",
    "rki_weekly['ar2_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(1)\n",
    "# rki_weekly['ar3_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(2)\n",
    "# rki_weekly['ar4_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(3)\n",
    "\n",
    "rki_weekly['target'] = np.log(rki_weekly['R0_this_week'])\n",
    "rki_weekly['auto_regr'] = np.log(rki_weekly['R0_last_week'])\n",
    "rki_weekly['auto_regr2'] = np.log(rki_weekly['ar2_'])\n",
    "# rki_weekly['auto_regr3'] = np.log(rki_weekly['ar3_'])\n",
    "# rki_weekly['auto_regr4'] = np.log(rki_weekly['ar4_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('processed_static_data.csv')\n",
    "mobility_features = pd.read_csv('mobility_features.csv')\n",
    "weather_features = pd.read_csv('weather_features.csv')\n",
    "all_data = rki_weekly.merge(static_data).merge(mobility_features[mobility_features.year==2020]).merge(weather_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating between day off and workday mobility features not productive as they correlate too highly anyway. I reverse it here, instead of in the feature creation, mostly due to the fact that new mobility feature creation would entail more beaurocracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_resum = ['Count_internal','Count_incoming']\n",
    "\n",
    "for f in features_to_resum:\n",
    "    for p in ['','_p_pop']:\n",
    "        var_work = f + '_workday' + p\n",
    "        var_off = f + '_day_off' + p\n",
    "        all_data[f + p] = all_data[var_work] + all_data[var_off]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timeseries features get used twice, once as is, once shifted back by one week, effectively creating an ARX(2,2) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_2_lag = ['Count_internal',\n",
    "       'Count_incoming', 'incoming_infected',\n",
    "       'Count_internal_p_pop',\n",
    "       'Count_incoming_p_pop',\n",
    "       'incoming_infected_p_pop',\n",
    "       'temperature', 'humidity', 'precipitation',\n",
    "       'sunshine', 'velocity',\n",
    "        'Count_internal_workday', 'Count_internal_day_off',\n",
    "       'Count_incoming_workday', 'Count_incoming_day_off', \n",
    "       'Count_internal_workday_p_pop', 'Count_internal_day_off_p_pop',\n",
    "       'Count_incoming_workday_p_pop', 'Count_incoming_day_off_p_pop',\n",
    "       'temperature_workday', 'humidity_workday', 'precipitation_workday',\n",
    "       'sunshine_workday', 'velocity_workday', 'direction_workday',\n",
    "       'temperature_day_off', 'humidity_day_off', 'precipitation_day_off',\n",
    "       'sunshine_day_off', 'velocity_day_off', 'direction_day_off']\n",
    "\n",
    "\n",
    "for var in vars_2_lag:\n",
    "    var_name = var + '_lag1'\n",
    "    all_data[var_name] = all_data.groupby('districtId')[var].shift(1)\n",
    "    \n",
    "    \n",
    "#additional 2 week lag no benefit at all:\n",
    "#     var_name = var + '_lag2'\n",
    "#     all_data[var_name] = all_data.groupby('districtId')[var].shift(2)\n",
    "#Thus, for exogenous influences, I take the original timeseries, and\n",
    "#the one-lagged, but no more.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we slice away the early weeks where there were too few cases, and the later weeks, which are outside our study's set scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_WEEK = 12\n",
    "LAST_WEEK = 38\n",
    "\n",
    "all_data = all_data[all_data.week_no.between(FIRST_WEEK, LAST_WEEK)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the cross-prediction function, which has as many decision parametrized as possible. See in-function description for details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_predict(data, how, model, X_cols, y_col, id_col = None, shuffle_mode = 'id', mod_params = None, mod_attrs_2_return = None, z_trim = None, random_state = 1):\n",
    "    \n",
    "    '''Function to cross-predict target variable across multiple groups.\n",
    "    Parameters:\n",
    "    data = Dataframe containing at least X_cols, y_col and the same (multi-)index as intended for the output, additional columns are dropped. Rows with missing values in any relevant column are dropped as well.\n",
    "    how = Either number of groups for cross-validation or string 's'/'student_resid' for studentized residuals, i.e. each observation as its own group, predicted by a model fitted on all data except the observation itself. Can be arbitrarily high, even higher than N of data, with resulting empty groups ignored. Setting \\'how\\'' to 1 omits cross-prediction, and fits and uses the model on the entire or only z-trimmed data set instead.\n",
    "    model = String corresponding to the name of an already imported model class. Only classes with .fit() and .predict() methods will work.\n",
    "    X_col = List of column names used as features in target prediction\n",
    "    y_col = Column name of target variable\n",
    "    id_col = Single column name or list of column names needed for re-merging the results to other dataframes. If None, the dataframe index, passed through from the data input, can still be used\n",
    "    shuffle_mode = 'id': all values with the same ID are in the same group. 'full': every data point is shuffled independently, so that e.g. the same district can have weekly values in the training set and the prediction set.\n",
    "    mod_params = optional dictionary with named parameters for the model, e.g. max_depth for RF. The function's random state is passed as random_state parameter automatically.\n",
    "    mod_attrs_2_return = String name or list of names of attributes of the fitted model objects to return, e.g. 'coef_' for linear regression weights. Returns a dictionary with attribute name(s) as key(s) and corresponding list of attribute values as value(s).\n",
    "    z_trim = optional threshold for extreme outliers to be removed from training data, to avoid overfitting on these outliers. All observations with target values with a z-score above this value will be dropped from the training data. Does not affect test data, so that potential positive deviance can still be found without hurting the model.\n",
    "    random_state = seed for RandomState object used in all RNG processes for re-producible results.\n",
    "    \n",
    "    Returns:\n",
    "    output_df = Dataframe containing predicted target values, actual target values and grouping IDs from the cross-prediction process. Indexed with the same variables as the data input.\n",
    "    accuracies = Dictionary with overall_R2, training_accuriacies and, if more than one value is predicted per iteration, the resulting out_of_sample_accuracies.\n",
    "    mod_attrs = dictionary of lists with the requested model attributes to return, keyed with the attribute name. Empty dictionary if mod_attrs_2_return is None.\n",
    "    \n",
    "    Note: Function is written for model objects with sklearn syntax, e.g. initializing without arguments, fitting in place.\n",
    "    Statsmodels uses different syntax, initializing with endogenous and exogenous variables, returning the fitted model. Instead of writing exception handling for all that, it's easier to just write an sklearn-esque version of the Statsmodels function if need be, like here for ordinary least squares regression:\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "    class sm_ols():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = sm.add_constant(X)\n",
    "        self.mod = sm.OLS(y,X).fit()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = sm.add_constant(X)\n",
    "        predictions = self.mod.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    from sklearn.metrics import r2_score, mean_absolute_error\n",
    "    \n",
    "    if id_col is not None:\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col,id_col]) #flatten, since X and id can \n",
    "    else:  #be either one string or a list of strings\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col])\n",
    "    df = data[all_cols].copy() #drop columns not needed\n",
    "    df.dropna(axis=0, inplace=True) #rowwise delete NAs \n",
    "    X = df[X_cols] #select predictors\n",
    "    y = df[y_col] #select target\n",
    "    n = df.shape[0] #get sample size (after NA removal)\n",
    "    rand = np.random.RandomState(random_state) #init random state\n",
    "    \n",
    "    train_accs = [] #init list of individual training accuracies\n",
    "    test_accs = [] #init list of individual test accuracies\n",
    "    mod_attrs = {}\n",
    "    if mod_attrs_2_return is not None: #init output for model attributes\n",
    "        if type(mod_attrs_2_return) == str: #if a single attribute name string,\n",
    "            mod_attrs[mod_attrs_2_return]=[] #the function will output a list of these attribute values\n",
    "        else: #else, we'll assume an iterable input, where each attribute will be\n",
    "            #used as the key in a dictionary of lists\n",
    "            for a in mod_attrs_2_return: #of attribute values that is initialised emptily\n",
    "                mod_attrs[a] = [] #here.\n",
    "    \n",
    "    output_df = pd.DataFrame() #init dataframe for output\n",
    "    pred_var_name = y_col + '_predicted' #define name for predicted target variable\n",
    "    \n",
    "    if shuffle_mode == 'full':#every row of the dataframe is treated independently\n",
    "        if how == 'student_resid' or how == 's': #if studentized residuals are wanted, \n",
    "            group = np.arange(n) #each observation is its own group\n",
    "        else:\n",
    "            try: #otherwise assign to specified number of groups\n",
    "                group = np.arange(n) % how\n",
    "                rand.shuffle(group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "            except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "    elif shuffle_mode == 'id': #all values of one id must be in the same group\n",
    "        if id_col is None:\n",
    "            raise ValueError('No ID cols to shuffle with!')\n",
    "        else:\n",
    "            unique_IDs = pd.DataFrame(df.groupby(id_col).size().reset_index()[id_col]) \n",
    "            #this gets all unique ID values for a single string, as well as all unique combinations for a list of strings\n",
    "            n_uniq = unique_IDs.shape[0] #number of unique ID(combination)s as length of group column to be created\n",
    "            \n",
    "            if how =='student_resid' or how == 's':\n",
    "                ID_group = np.arange(n_uniq) #each unique ID(combination) gets its own group\n",
    "            else:\n",
    "                try: #otherwise assign to specified number of groups\n",
    "                    ID_group = np.arange(n_uniq) % how\n",
    "                    rand.shuffle(ID_group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "                except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                    raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "            \n",
    "            unique_IDs['group'] = ID_group #add group to the dataframe of unique IDs so that\n",
    "            df_with_group = pd.merge(df, unique_IDs) #we can merge it with the dataframe to get the \n",
    "            group = np.array(df_with_group['group']) #array matching each row of the full dataset to its respective group\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('shuffle mode must be either \\'id\\' or \\'full\\'!')\n",
    "            \n",
    "    for g in np.unique(group):#iterate over groups\n",
    "        \n",
    "        if how == 1: #if only one group, \n",
    "            X_train, y_train = X, y #train on entire dataset.\n",
    "            X_test, y_test = X, y #this test set would happen anyway since all data is in group g=0, included here only as failsafe.\n",
    "        else:\n",
    "            X_train = X[group != g] #train model on all data NOT in the group\n",
    "            y_train = y[group != g]\n",
    "                \n",
    "            X_test = X[group == g] #test/predict in group data\n",
    "            y_test = y[group == g]\n",
    "\n",
    "            \n",
    "        if z_trim is not None: #if extreme values should be trimmed from training data\n",
    "            m_y = np.mean(y_train) #calc train target mean\n",
    "            s_y = np.std(y_train) #and SD\n",
    "            z_y = (y_train - m_y) / s_y #for z-values\n",
    "            X_train = X_train[abs(z_y) <= z_trim] #and cut all observations with target values\n",
    "            y_train = y_train[abs(z_y) <= z_trim] #more extreme than z_trim threshold\n",
    "        \n",
    "        #defining model by trying to pass string input as (previously imported) model class name.\n",
    "        #Both passing and not passing call brackets will work.\n",
    "        try: \n",
    "            mod = eval(model+'()')\n",
    "        except:\n",
    "            try:\n",
    "                mod = eval(model)\n",
    "            except:\n",
    "                raise ValueError('\\'model\\' parameter could not be interpreted as model class. Check whether you have imported the corresponding class.')\n",
    "\n",
    "        setattr(mod, 'random_state', rand)#if model uses RNG, passing the function's RandomState object here. Will be passively ignored by e.g. LinearRegression().\n",
    "\n",
    "        if mod_params is not None: #if model params are provided as a dict\n",
    "            for k, v in mod_params.items(): #iterate over dict and\n",
    "                setattr(mod,k,v) #set attributes of mod accordingly\n",
    "                #meaningless attributes are added without effect, e.g.\n",
    "                #max_depth for LinearRegression will be set without error\n",
    "                #because it is simply ignored\n",
    "        \n",
    "        mod.fit(X_train, y_train) #fit model to training data\n",
    "        \n",
    "        train_accs.append(r2_score(y_train, mod.predict(X_train))) #save model performance for training set\n",
    "        \n",
    "        y_pred = mod.predict(X_test) #predict target variable for data in current group\n",
    "        #this will be the prediction that 'counts' for this data, as it is the only prediction\n",
    "        #for this data where the model has not seen this data before\n",
    "        \n",
    "        if len(y_pred) > 1: #y_pred will be 1 long when studentized residuals are chosen or when groups become too small\n",
    "            test_accs.append(r2_score(y_test, y_pred)) #if there are enough (2) observations, save test performance as well\n",
    "        \n",
    "        group_output = pd.DataFrame({pred_var_name:y_pred, #the predicted target\n",
    "                                    y_col:y_test, #values, the actual target values and the\n",
    "                                    'grouping_id':g}) #grouping IDs for potential trouble shooting\n",
    "        \n",
    "        if id_col is None: #if no ID columns are supposed to be passed through,\n",
    "            pass #no action necessary\n",
    "        elif type(id_col) == str: #alternatively, if one col name string is given,\n",
    "            group_output[id_col] = df[id_col][group == g] #add this column to group output\n",
    "        else: #else, id_col is assumed to be iterable (a list). id_cols that are neither strings, nor iterables\n",
    "            for i in id_col: #will throw a non-iterable error. Here, we iterate over the list and\n",
    "                group_output[i] = df[i][group == g] #add each col to output\n",
    "        \n",
    "        output_df = pd.concat([output_df, group_output]) #add output for current group to overall output\n",
    "        \n",
    "        if mod_attrs_2_return is None: #if no mod attributes are wanted,\n",
    "            pass #no action is necessary\n",
    "        elif type(mod_attrs_2_return) == str: #if one attribute name string is given,\n",
    "            mod_attrs[mod_attrs_2_return].append(getattr(mod, mod_attrs_2_return)) #add the current models value to list\n",
    "        else: #else, input is assumed to be iterable (a list). If neither string, nor iterable, an error is raised about\n",
    "            for a in mod_attrs_2_return: #it not being iterable. Here, we iterate over the attributes and append\n",
    "                mod_attrs[a].append(getattr(mod, a)) #each attribute to the right list in the attribute dict\n",
    "        \n",
    "    output_df.sort_index(inplace=True)#revert order to index ordering, as it is now sorted by group first\n",
    "    #If the input dataframe was sorted by its index, this should result in \n",
    "    #the same order as the input dataframe, apart from the dropped rows due to missing values.\n",
    "    total_r2 = r2_score(output_df[y_col],output_df[pred_var_name]) #calc R2 for all data combined\n",
    "    total_mae = mean_absolute_error(output_df[y_col],output_df[pred_var_name])\n",
    "    print('Overall, the cross-predictions accounted for {:.2%} of the target variance.'.format(total_r2))\n",
    "    print('The Mean Absolute Error was {:.5}'.format(total_mae))  \n",
    "    \n",
    "    accuracies = {'overall_R2': total_r2,\n",
    "                 'training_accuracies':train_accs}\n",
    "    if len(test_accs) > 0:\n",
    "        accuracies['out_of_sample_accuracies']=test_accs\n",
    "                                                                                            \n",
    "    return output_df, accuracies, mod_attrs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of all predictors/features to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_variables = ['auto_regr',\n",
    "                       'auto_regr2',\n",
    "                       'Count_internal_p_pop',\n",
    "                       'Count_internal_p_pop_lag1',\n",
    "                       'Count_incoming_p_pop',\n",
    "                       'Count_incoming_p_pop_lag1',\n",
    "                       'incoming_infected_p_pop',\n",
    "                       'incoming_infected_p_pop_lag1',\n",
    "                       'temperature',\n",
    "                       'temperature_lag1',\n",
    "                       'humidity',\n",
    "                       'humidity_lag1',\n",
    "                       'precipitation',\n",
    "                       'precipitation_lag1',\n",
    "                       'sunshine',\n",
    "                       'sunshine_lag1'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: The standard sklearn Huber regression does not yield standardized regression weights, but for deviance purposes, I also don't want to z-standardize my target (or features). Thus, I create a class here that does the standardizing and de-standardizing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "class standard_huber():\n",
    "    def __init__(self, epsilon = 1.35):#1.35 is also the default value in sklearn, and the one proposed by Mr Huber himself\n",
    "        self.x_sclr = StandardScaler()\n",
    "        self.y_sclr = StandardScaler()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.mod = HuberRegressor(epsilon=self.epsilon)\n",
    "        X_scld = self.x_sclr.fit_transform(X)\n",
    "        y_rshp = np.array(y).reshape(-1,1)\n",
    "        y_scld = self.y_sclr.fit_transform(y_rshp).reshape(-1,)\n",
    "        self.mod.fit(X_scld,y_scld)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        \n",
    "        X_scld = self.x_sclr.transform(X)\n",
    "        pred_y_scld = self.mod.predict(X_scld)\n",
    "        predictions = self.y_sclr.inverse_transform(pred_y_scld).reshape(-1,)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cross-prediction:\n",
    "[EDIT:] In the end, we did not end up needing cross prediction, since the Huber is so very robust anyway. To keep the pipeline intact, I still call the cross prediction function, but with 1 as the how-argument, i.e. with fitting and predicting on one single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, the cross-predictions accounted for 14.76% of the target variance.\n",
      "The Mean Absolute Error was 0.55442\n"
     ]
    }
   ],
   "source": [
    "predictions, accs, mod_attrs = cross_predict(data = all_data,\n",
    "                                             X_cols = predictor_variables,\n",
    "                                             y_col = 'target',\n",
    "                                             id_col = 'districtId',\n",
    "                                             how = 1,\n",
    "                                             shuffle_mode='id',\n",
    "                                             model = 'standard_huber',\n",
    "                                             mod_attrs_2_return = 'coef_',\n",
    "                                             z_trim = None,\n",
    "                                             mod_params={'epsilon':1.35})#see above, 1.35 is default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the deviance, and print the correlation of predicted and actual target values, mostly as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.38430108]\n",
      " [0.38430108 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "predictions['deviance'] = predictions['target_predicted'] - predictions['target']\n",
    "print(np.corrcoef(predictions['target_predicted'], predictions['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge it all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, predictions, left_index=True, right_index=True, suffixes=('','_del')).drop(columns=['districtId_del','target_del'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('all_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity-check regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model is not corrupted by outliers, there should not be any correlation between any predictor and the deviance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All correlations between residuals and predictors were between -0.01 and 0.02\n"
     ]
    }
   ],
   "source": [
    "corr=all_data[predictor_variables+['deviance']].corr()\n",
    "\n",
    "print('All correlations between residuals and predictors were between {:.2f} and {:.2f}'.format(corr.deviance.iloc[:-1].min(),corr.deviance.iloc[:-1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the residuals are centered around zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2538735e0c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data.deviance.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predicted and actual values against each other. You see the larger spike of zeroes in the target, which mostly result from one being the 'default' value for the reproduction factors, thus leading to logs of zero. Since these don't create variance (mean is about zero), model performance is actually worse when dropping these values, so I'd keep these values, rather than working with a gappy timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x253874add88>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.jointplot(predictions.target, predictions.target_predicted,kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse regression weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dict = {}\n",
    "weight_dict = {}\n",
    "for i, var in enumerate(predictor_variables):\n",
    "    \n",
    "    attr_dict[var] = []\n",
    "          \n",
    "    for a in mod_attrs['coef_']:\n",
    "        attr_dict[var].append(a[i])\n",
    "    \n",
    "    weight_dict[var] = {'mean':np.mean(attr_dict[var]),\n",
    "                       'sd':np.std(attr_dict[var])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_dict = {'auto_regr':'Log-R0 t-1',\n",
    "'auto_regr2':'Log-R0 t-2',\n",
    "'Count_internal_p_pop':'Internal travel per capita week t',\n",
    "'Count_internal_p_pop_lag1':'Internal travel per capita week t-1',\n",
    "'Count_incoming_p_pop':'Incoming travel per capita week t',\n",
    "'Count_incoming_p_pop_lag1':'Incoming travel per capita week t-1',\n",
    "'incoming_infected_p_pop':'Incoming infection load per capita week t',\n",
    "'incoming_infected_p_pop_lag1':'Incoming infection load per capita week t-1',\n",
    "'temperature':'Average temperature week t',\n",
    "'temperature_lag1':'Average temperature week t-1',\n",
    "'humidity':'Average humidity week t',\n",
    "'humidity_lag1':'Average humidity week t-1',\n",
    "'precipitation':'Average precipitation week t',\n",
    "'precipitation_lag1':'Average precipitation week t-1',\n",
    "'sunshine':'Average daily sunshine week t',\n",
    "'sunshine_lag1':'Average daily sunshine week t-1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(weight_dict).T.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>auto_regr</th>\n",
       "      <td>-0.293</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auto_regr2</th>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count_internal_p_pop</th>\n",
       "      <td>-0.337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count_internal_p_pop_lag1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count_incoming_p_pop</th>\n",
       "      <td>-0.668</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count_incoming_p_pop_lag1</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incoming_infected_p_pop</th>\n",
       "      <td>0.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incoming_infected_p_pop_lag1</th>\n",
       "      <td>-0.267</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature_lag1</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>0.018</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity_lag1</th>\n",
       "      <td>0.016</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipitation</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precipitation_lag1</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunshine</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunshine_lag1</th>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mean   sd\n",
       "auto_regr                    -0.293  0.0\n",
       "auto_regr2                   -0.081  0.0\n",
       "Count_internal_p_pop         -0.337  0.0\n",
       "Count_internal_p_pop_lag1     0.350  0.0\n",
       "Count_incoming_p_pop         -0.668  0.0\n",
       "Count_incoming_p_pop_lag1     0.694  0.0\n",
       "incoming_infected_p_pop       0.247  0.0\n",
       "incoming_infected_p_pop_lag1 -0.267  0.0\n",
       "temperature                   0.080  0.0\n",
       "temperature_lag1              0.067  0.0\n",
       "humidity                      0.018  0.0\n",
       "humidity_lag1                 0.016  0.0\n",
       "precipitation                -0.012  0.0\n",
       "precipitation_lag1            0.043  0.0\n",
       "sunshine                      0.036  0.0\n",
       "sunshine_lag1                -0.106  0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.index=weight_df.index.map(predictor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df['mean'].to_csv('regression_weights.csv',header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall, the cross-predictions accounted for 14.35% of the target variance.\n",
      "The Mean Absolute Error was 0.55581\n",
      "Overall, the cross-predictions accounted for 14.30% of the target variance.\n",
      "The Mean Absolute Error was 0.5559\n",
      "Overall, the cross-predictions accounted for 14.15% of the target variance.\n",
      "The Mean Absolute Error was 0.55666\n",
      "Overall, the cross-predictions accounted for 14.38% of the target variance.\n",
      "The Mean Absolute Error was 0.55589\n",
      "Overall, the cross-predictions accounted for 14.41% of the target variance.\n",
      "The Mean Absolute Error was 0.55574\n",
      "Overall, the cross-predictions accounted for 14.29% of the target variance.\n",
      "The Mean Absolute Error was 0.55631\n",
      "Overall, the cross-predictions accounted for 14.30% of the target variance.\n",
      "The Mean Absolute Error was 0.55586\n",
      "Overall, the cross-predictions accounted for 14.47% of the target variance.\n",
      "The Mean Absolute Error was 0.55579\n",
      "Overall, the cross-predictions accounted for 14.31% of the target variance.\n",
      "The Mean Absolute Error was 0.55621\n",
      "Overall, the cross-predictions accounted for 14.37% of the target variance.\n",
      "The Mean Absolute Error was 0.5559\n",
      "Overall, the cross-predictions accounted for 14.39% of the target variance.\n",
      "The Mean Absolute Error was 0.55578\n",
      "Overall, the cross-predictions accounted for 14.56% of the target variance.\n",
      "The Mean Absolute Error was 0.55532\n",
      "Overall, the cross-predictions accounted for 14.49% of the target variance.\n",
      "The Mean Absolute Error was 0.55564\n",
      "Overall, the cross-predictions accounted for 14.36% of the target variance.\n",
      "The Mean Absolute Error was 0.55606\n",
      "Overall, the cross-predictions accounted for 14.22% of the target variance.\n",
      "The Mean Absolute Error was 0.55653\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(15):\n",
    "    predictions, accs, mod_attrs = cross_predict(data = all_data,\n",
    "                                             X_cols = predictor_variables,\n",
    "                                             y_col = 'target',\n",
    "                                             id_col = 'districtId',\n",
    "                                             how = 2,\n",
    "                                             shuffle_mode='id',\n",
    "                                             model = 'standard_huber',\n",
    "                                             mod_attrs_2_return = 'coef_',\n",
    "                                             z_trim = None,\n",
    "                                                 random_state=i,\n",
    "                                             mod_params={'epsilon':1.35})#see above, 1.35 is default\n",
    "    if i == 0:\n",
    "        df = predictions.copy()\n",
    "    else:\n",
    "        df = df.merge(predictions, left_index=True, right_index=True,suffixes=('',i))\n",
    "df=df.filter(like='target_predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_predicted</th>\n",
       "      <th>target_predicted1</th>\n",
       "      <th>target_predicted2</th>\n",
       "      <th>target_predicted3</th>\n",
       "      <th>target_predicted4</th>\n",
       "      <th>target_predicted5</th>\n",
       "      <th>target_predicted6</th>\n",
       "      <th>target_predicted7</th>\n",
       "      <th>target_predicted8</th>\n",
       "      <th>target_predicted9</th>\n",
       "      <th>target_predicted10</th>\n",
       "      <th>target_predicted11</th>\n",
       "      <th>target_predicted12</th>\n",
       "      <th>target_predicted13</th>\n",
       "      <th>target_predicted14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>target_predicted</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted2</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted3</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted4</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted5</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted6</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted7</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted8</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted9</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted10</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted11</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted12</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted13</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_predicted14</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    target_predicted  target_predicted1  target_predicted2  \\\n",
       "target_predicted                1.00               0.99               0.99   \n",
       "target_predicted1               0.99               1.00               0.99   \n",
       "target_predicted2               0.99               0.99               1.00   \n",
       "target_predicted3               0.99               0.99               0.99   \n",
       "target_predicted4               0.99               0.99               0.99   \n",
       "target_predicted5               0.99               0.99               0.99   \n",
       "target_predicted6               0.99               0.99               0.99   \n",
       "target_predicted7               0.99               0.99               0.99   \n",
       "target_predicted8               0.99               0.99               0.99   \n",
       "target_predicted9               0.99               0.99               0.99   \n",
       "target_predicted10              0.99               0.99               0.99   \n",
       "target_predicted11              0.99               0.99               0.99   \n",
       "target_predicted12              0.99               0.99               0.99   \n",
       "target_predicted13              0.99               0.99               0.99   \n",
       "target_predicted14              0.99               0.99               0.99   \n",
       "\n",
       "                    target_predicted3  target_predicted4  target_predicted5  \\\n",
       "target_predicted                 0.99               0.99               0.99   \n",
       "target_predicted1                0.99               0.99               0.99   \n",
       "target_predicted2                0.99               0.99               0.99   \n",
       "target_predicted3                1.00               0.99               0.99   \n",
       "target_predicted4                0.99               1.00               0.99   \n",
       "target_predicted5                0.99               0.99               1.00   \n",
       "target_predicted6                0.99               0.99               0.99   \n",
       "target_predicted7                0.99               0.99               0.99   \n",
       "target_predicted8                0.99               0.99               0.99   \n",
       "target_predicted9                0.99               0.99               0.99   \n",
       "target_predicted10               0.99               0.99               0.99   \n",
       "target_predicted11               0.99               0.99               0.99   \n",
       "target_predicted12               0.99               0.99               0.99   \n",
       "target_predicted13               0.99               0.99               0.99   \n",
       "target_predicted14               0.99               0.99               0.99   \n",
       "\n",
       "                    target_predicted6  target_predicted7  target_predicted8  \\\n",
       "target_predicted                 0.99               0.99               0.99   \n",
       "target_predicted1                0.99               0.99               0.99   \n",
       "target_predicted2                0.99               0.99               0.99   \n",
       "target_predicted3                0.99               0.99               0.99   \n",
       "target_predicted4                0.99               0.99               0.99   \n",
       "target_predicted5                0.99               0.99               0.99   \n",
       "target_predicted6                1.00               0.99               0.99   \n",
       "target_predicted7                0.99               1.00               0.99   \n",
       "target_predicted8                0.99               0.99               1.00   \n",
       "target_predicted9                0.99               0.99               0.99   \n",
       "target_predicted10               0.99               0.99               0.99   \n",
       "target_predicted11               0.99               0.99               0.99   \n",
       "target_predicted12               0.99               0.99               0.99   \n",
       "target_predicted13               0.99               0.99               0.99   \n",
       "target_predicted14               0.99               0.99               0.99   \n",
       "\n",
       "                    target_predicted9  target_predicted10  target_predicted11  \\\n",
       "target_predicted                 0.99                0.99                0.99   \n",
       "target_predicted1                0.99                0.99                0.99   \n",
       "target_predicted2                0.99                0.99                0.99   \n",
       "target_predicted3                0.99                0.99                0.99   \n",
       "target_predicted4                0.99                0.99                0.99   \n",
       "target_predicted5                0.99                0.99                0.99   \n",
       "target_predicted6                0.99                0.99                0.99   \n",
       "target_predicted7                0.99                0.99                0.99   \n",
       "target_predicted8                0.99                0.99                0.99   \n",
       "target_predicted9                1.00                0.99                0.99   \n",
       "target_predicted10               0.99                1.00                0.99   \n",
       "target_predicted11               0.99                0.99                1.00   \n",
       "target_predicted12               0.99                0.99                0.99   \n",
       "target_predicted13               0.99                0.99                0.99   \n",
       "target_predicted14               0.99                0.99                0.99   \n",
       "\n",
       "                    target_predicted12  target_predicted13  target_predicted14  \n",
       "target_predicted                  0.99                0.99                0.99  \n",
       "target_predicted1                 0.99                0.99                0.99  \n",
       "target_predicted2                 0.99                0.99                0.99  \n",
       "target_predicted3                 0.99                0.99                0.99  \n",
       "target_predicted4                 0.99                0.99                0.99  \n",
       "target_predicted5                 0.99                0.99                0.99  \n",
       "target_predicted6                 0.99                0.99                0.99  \n",
       "target_predicted7                 0.99                0.99                0.99  \n",
       "target_predicted8                 0.99                0.99                0.99  \n",
       "target_predicted9                 0.99                0.99                0.99  \n",
       "target_predicted10                0.99                0.99                0.99  \n",
       "target_predicted11                0.99                0.99                0.99  \n",
       "target_predicted12                1.00                0.99                0.99  \n",
       "target_predicted13                0.99                1.00                0.99  \n",
       "target_predicted14                0.99                0.99                1.00  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr().round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
