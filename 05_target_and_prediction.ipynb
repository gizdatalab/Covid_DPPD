{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xpred import cross_predict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rki_weekly = pd.read_csv('rki_weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rki_weekly = rki_weekly[rki_weekly.week_no.between(12,38,inclusive=True)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_as_default = True #False would turn div-by-zero and zero R0s into NaNs\n",
    "#model performance decreases when False, but distribution of target has less of\n",
    "#a spike at 0. What's your opinion here?\n",
    "\n",
    "rki_weekly['infs_last_week'] = rki_weekly.groupby('districtId')['AnzahlFall'].shift(1)\n",
    "\n",
    "if one_as_default:\n",
    "    rki_weekly['R0_last_week'] = (rki_weekly['AnzahlFall'] + 1) / (rki_weekly['infs_last_week'] + 1)\n",
    "else:\n",
    "    rki_weekly['R0_last_week'] = (rki_weekly['AnzahlFall']) / (rki_weekly['infs_last_week'])\n",
    "    rki_weekly['R0_last_week'].replace([np.inf, 0],np.nan,inplace=True)\n",
    "\n",
    "rki_weekly['R0_this_week'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(-1)\n",
    "rki_weekly['ar2_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(1)\n",
    "rki_weekly['ar3_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(2)\n",
    "rki_weekly['ar4_'] = rki_weekly.groupby('districtId')['R0_last_week'].shift(3)\n",
    "\n",
    "rki_weekly['target'] = np.log(rki_weekly['R0_this_week'])\n",
    "rki_weekly['auto_regr'] = np.log(rki_weekly['R0_last_week'])\n",
    "rki_weekly['auto_regr2'] = np.log(rki_weekly['ar2_'])\n",
    "rki_weekly['auto_regr3'] = np.log(rki_weekly['ar3_'])\n",
    "rki_weekly['auto_regr4'] = np.log(rki_weekly['ar4_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv('processed_static_data.csv')\n",
    "mobility_features = pd.read_csv('mobility_features.csv')\n",
    "internationality  = pd.read_csv('internationality.csv')\n",
    "weather_features = pd.read_csv('weather_features.csv')#weather is still buggy, not to be trusted over a bridge.\n",
    "all_data = rki_weekly.merge(static_data).merge(mobility_features[mobility_features.year==2020]).merge(internationality).merge(weather_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_predict(data, how, model, X_cols, y_col, id_col = None, shuffle_mode = 'id', mod_params = None, mod_attrs_2_return = None, z_trim = None, random_state = 1):\n",
    "    \n",
    "    '''Function to cross-predict target variable across multiple groups.\n",
    "    Parameters:\n",
    "    data = Dataframe containing at least X_cols, y_col and the same (multi-)index as intended for the output, additional columns are dropped. Rows with missing values in any relevant column are dropped as well.\n",
    "    how = Either number of groups for cross-validation or string 's'/'student_resid' for studentized residuals, i.e. each observation as its own group, predicted by a model fitted on all data except the observation itself. Can be arbitrarily high, even higher than N of data, with resulting empty groups ignored. Setting \\'how\\'' to 1 omits cross-prediction, and fits and uses the model on the entire or only z-trimmed data set instead.\n",
    "    model = String corresponding to the name of an already imported model class. Only classes with .fit() and .predict() methods will work.\n",
    "    X_col = List of column names used as features in target prediction\n",
    "    y_col = Column name of target variable\n",
    "    id_col = Single column name or list of column names needed for re-merging the results to other dataframes. If None, the dataframe index, passed through from the data input, can still be used\n",
    "    shuffle_mode = 'id': all values with the same ID are in the same group. 'full': every data point is shuffled independently, so that e.g. the same district can have weekly values in the training set and the prediction set.\n",
    "    mod_params = optional dictionary with named parameters for the model, e.g. max_depth for RF. The function's random state is passed as random_state parameter automatically.\n",
    "    mod_attrs_2_return = String name or list of names of attributes of the fitted model objects to return, e.g. 'coef_' for linear regression weights. Returns a dictionary with attribute name(s) as key(s) and corresponding list of attribute values as value(s).\n",
    "    z_trim = optional threshold for extreme outliers to be removed from training data, to avoid overfitting on these outliers. All observations with target values with a z-score above this value will be dropped from the training data. Does not affect test data, so that potential positive deviance can still be found without hurting the model.\n",
    "    random_state = seed for RandomState object used in all RNG processes for re-producible results.\n",
    "    \n",
    "    Returns:\n",
    "    output_df = Dataframe containing predicted target values, actual target values and grouping IDs from the cross-prediction process. Indexed with the same variables as the data input.\n",
    "    accuracies = Dictionary with overall_R2, training_accuriacies and, if more than one value is predicted per iteration, the resulting out_of_sample_accuracies.\n",
    "    mod_attrs = dictionary of lists with the requested model attributes to return, keyed with the attribute name. Empty dictionary if mod_attrs_2_return is None.\n",
    "    \n",
    "    Note: Function is written for model objects with sklearn syntax, e.g. initializing without arguments, fitting in place.\n",
    "    Statsmodels uses different syntax, initializing with endogenous and exogenous variables, returning the fitted model. Instead of writing exception handling for all that, it's easier to just write an sklearn-esque version of the Statsmodels function if need be, like here for ordinary least squares regression:\n",
    "    \n",
    "    import statsmodels.api as sm\n",
    "    class sm_ols():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X = sm.add_constant(X)\n",
    "        self.mod = sm.OLS(y,X).fit()\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X = sm.add_constant(X)\n",
    "        predictions = self.mod.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if id_col is not None:\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col,id_col]) #flatten, since X and id can \n",
    "    else:  #be either one string or a list of strings\n",
    "        all_cols = pd.core.common.flatten([X_cols,y_col])\n",
    "    df = data[all_cols].copy() #drop columns not needed\n",
    "    df.dropna(axis=0, inplace=True) #rowwise delete NAs \n",
    "    X = df[X_cols] #select predictors\n",
    "    y = df[y_col] #select target\n",
    "    n = df.shape[0] #get sample size (after NA removal)\n",
    "    rand = np.random.RandomState(random_state) #init random state\n",
    "    \n",
    "    train_accs = [] #init list of individual training accuracies\n",
    "    test_accs = [] #init list of individual test accuracies\n",
    "    mod_attrs = {}\n",
    "    if mod_attrs_2_return is not None: #init output for model attributes\n",
    "        if type(mod_attrs_2_return) == str: #if a single attribute name string,\n",
    "            mod_attrs[mod_attrs_2_return]=[] #the function will output a list of these attribute values\n",
    "        else: #else, we'll assume an iterable input, where each attribute will be\n",
    "            #used as the key in a dictionary of lists\n",
    "            for a in mod_attrs_2_return: #of attribute values that is initialised emptily\n",
    "                mod_attrs[a] = [] #here.\n",
    "    \n",
    "    output_df = pd.DataFrame() #init dataframe for output\n",
    "    pred_var_name = y_col + '_predicted' #define name for predicted target variable\n",
    "    \n",
    "    if shuffle_mode == 'full':#every row of the dataframe is treated independently\n",
    "        if how == 'student_resid' or how == 's': #if studentized residuals are wanted, \n",
    "            group = np.arange(n) #each observation is its own group\n",
    "        else:\n",
    "            try: #otherwise assign to specified number of groups\n",
    "                group = np.arange(n) % how\n",
    "                rand.shuffle(group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "            except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "    elif shuffle_mode == 'id': #all values of one id must be in the same group\n",
    "        if id_col is None:\n",
    "            raise ValueError('No ID cols to shuffle with!')\n",
    "        else:\n",
    "            unique_IDs = pd.DataFrame(df.groupby(id_col).size().reset_index()[id_col]) \n",
    "            #this gets all unique ID values for a single string, as well as all unique combinations for a list of strings\n",
    "            n_uniq = unique_IDs.shape[0] #number of unique ID(combination)s as length of group column to be created\n",
    "            \n",
    "            if how =='student_resid' or how == 's':\n",
    "                ID_group = np.arange(n_uniq) #each unique ID(combination) gets its own group\n",
    "            else:\n",
    "                try: #otherwise assign to specified number of groups\n",
    "                    ID_group = np.arange(n_uniq) % how\n",
    "                    rand.shuffle(ID_group) #shuffle to avoid e.g. Bundesland effects due to districtId            \n",
    "                except: #catch all cases where this fails (i.e. non-int, non-positive)\n",
    "                #demanding more groups than cases is not problematic, since some groups will be empty and thus ignored,\n",
    "                #so they are not caught here\n",
    "                    raise ValueError('Parameter \\'how\\' needs to be positive integer or \\'student_resid\\' or \\'s\\'')\n",
    "            \n",
    "            unique_IDs['group'] = ID_group #add group to the dataframe of unique IDs so that\n",
    "            df_with_group = pd.merge(df, unique_IDs) #we can merge it with the dataframe to get the \n",
    "            group = np.array(df_with_group['group']) #array matching each row of the full dataset to its respective group\n",
    "            \n",
    "    else:\n",
    "        raise ValueError('shuffle mode must be either \\'id\\' or \\'full\\'!')\n",
    "            \n",
    "    for g in np.unique(group):#iterate over groups\n",
    "        \n",
    "        if how == 1: #if only one group, \n",
    "            X_train, y_train = X, y #train on entire dataset.\n",
    "            X_test, y_test = X, y #this would happen anyway since all data is in group g=0, included here only as failsafe.\n",
    "        else:\n",
    "            X_train = X[group != g] #train model on all data NOT in the group\n",
    "            y_train = y[group != g]\n",
    "                \n",
    "            X_test = X[group == g] #test/predict in group data\n",
    "            y_test = y[group == g]\n",
    "\n",
    "            \n",
    "        if z_trim is not None: #if extreme values should be trimmed from training data\n",
    "            m_y = np.mean(y_train) #calc train target mean\n",
    "            s_y = np.std(y_train) #and SD\n",
    "            z_y = (y_train - m_y) / s_y #for z-values\n",
    "            X_train = X_train[abs(z_y) <= z_trim] #and cut all observations with target values\n",
    "            y_train = y_train[abs(z_y) <= z_trim] #more extreme than z_trim threshold\n",
    "        \n",
    "        #defining model by trying to pass string input as (previously imported) model class name.\n",
    "        #Both passing and not passing call brackets will work.\n",
    "        try: \n",
    "            mod = eval(model+'()')\n",
    "        except:\n",
    "            try:\n",
    "                mod = eval(model)\n",
    "            except:\n",
    "                raise ValueError('\\'model\\' parameter could not be interpreted as model class. Check whether you have imported the corresponding class.')\n",
    "\n",
    "        setattr(mod, 'random_state', rand)#if model uses RNG, passing the function's RandomState object here. Will be passively ignored by e.g. LinearRegression().\n",
    "\n",
    "        if mod_params is not None: #if model params are provided as a dict\n",
    "            for k, v in mod_params.items(): #iterate over dict and\n",
    "                setattr(mod,k,v) #set attributes of mod accordingly\n",
    "                #meaningless attributes are added without effect, e.g.\n",
    "                #max_depth for LinearRegression will be set without error\n",
    "                #because it is simply ignored\n",
    "        \n",
    "        mod.fit(X_train, y_train) #fit model to training data\n",
    "\n",
    "        from sklearn.metrics import r2_score\n",
    "\n",
    "        train_accs.append(r2_score(y_train, mod.predict(X_train))) #save model performance for training set\n",
    "        \n",
    "        y_pred = mod.predict(X_test) #predict target variable for data in current group\n",
    "        #this will be the prediction that 'counts' for this data, as it is the only prediction\n",
    "        #for this data where the model has not seen this data before\n",
    "        \n",
    "        if len(y_pred) > 1: #y_pred will be 1 when studentized residuals are chosen or when groups become too small\n",
    "            test_accs.append(r2_score(y_test, y_pred)) #if there are enough (2) observations, save test performance as well\n",
    "            \n",
    "        group_output = pd.DataFrame({pred_var_name:y_pred, #the predicted target\n",
    "                                    y_col:y_test, #values, the actual target values and the\n",
    "                                    'grouping_id':g}) #grouping IDs for potential trouble shooting\n",
    "        if id_col is None: #if no ID columns are supposed to be passed through,\n",
    "            pass #no action necessary\n",
    "        elif type(id_col) == str: #alternatively, if one col name string is given,\n",
    "            group_output[id_col] = df[id_col][group == g] #add this column to group output\n",
    "        else: #else, id_col is assumed to be iterable (a list). id_cols that are neither strings, nor iterables\n",
    "            for i in id_col: #will throw a non-iterable error. Here, we iterate over the list and\n",
    "                group_output[i] = df[i][group == g] #add each col to output\n",
    "        \n",
    "        output_df = pd.concat([output_df, group_output]) #add output for current group to overall output\n",
    "        \n",
    "        if mod_attrs_2_return is None: #if no mod attributes are wanted,\n",
    "            pass #no action is necessary\n",
    "        elif type(mod_attrs_2_return) == str: #if one attribute name string is given,\n",
    "            mod_attrs[mod_attrs_2_return].append(getattr(mod, mod_attrs_2_return)) #add the current models value to list\n",
    "        else: #else, input is assumed to be iterable (a list). If neither string, nor iterable, an error is raised about\n",
    "            for a in mod_attrs_2_return: #it not being iterable. Here, we iterate over the attributes and append\n",
    "                mod_attrs[a].append(getattr(mod, a)) #each attribute to the right list in the attribute dict\n",
    "        \n",
    "    output_df.sort_index(inplace=True)#revert order to index ordering, as it is now sorted by group first\n",
    "    #If the input dataframe was sorted by its index, this should result in \n",
    "    #the same order as the input dataframe, apart from the dropped rows due to missing values.\n",
    "    total_r2 = r2_score(output_df[y_col],output_df[pred_var_name]) #calc R2 for all data combined\n",
    "       \n",
    "    print('Overall, the cross-predictions accounted for {:.2%} of the target variance.'.format(total_r2))  \n",
    "    \n",
    "    accuracies = {'overall_R2': total_r2,\n",
    "                 'training_accuracies':train_accs}\n",
    "    if len(test_accs) > 0:\n",
    "        accuracies['out_of_sample_accuracies']=test_accs\n",
    "                                                                                            \n",
    "    return output_df, accuracies, mod_attrs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class standard_linreg():\n",
    "    def __init__(self):\n",
    "        self.sclr = StandardScaler()\n",
    "        self.mod = LinearRegression()\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        X_scl = self.sclr.fit_transform(X)\n",
    "        self.mod.fit(X_scl,y)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        X_csl = self.sclr.transform(X)\n",
    "        predictions = self.mod.predict(X_csl)\n",
    "        return predictions\n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.mod.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_2_lag = ['Count_internal_workday', 'Count_internal_day_off',\n",
    "       'Count_incoming_workday', 'Count_incoming_day_off', 'incoming_infected',\n",
    "       'Count_internal_workday_p_pop', 'Count_internal_day_off_p_pop',\n",
    "       'Count_incoming_workday_p_pop', 'Count_incoming_day_off_p_pop',\n",
    "       'incoming_infected_p_pop',\n",
    "       'temperature_workday', 'humidity_workday', 'precipitation_workday',\n",
    "       'sunshine_workday', 'velocity_workday', 'direction_workday',\n",
    "       'temperature_day_off', 'humidity_day_off', 'precipitation_day_off',\n",
    "       'sunshine_day_off', 'velocity_day_off', 'direction_day_off']\n",
    "\n",
    "for var in vars_2_lag:\n",
    "    var_name = var + '_lag1'\n",
    "    all_data[var_name] = all_data.groupby('districtId')[var].shift(1)\n",
    "    #additional 2 week lag no benefit at all:\n",
    "#     var_name = var + '_lag2'\n",
    "#     all_data[var_name] = all_data.groupby('districtId')[var].shift(2)\n",
    "#Thus, for exogenous influences, I'd take the original timeseries, and\n",
    "#the one-lagged, but no more.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_variables = ['auto_regr','auto_regr2',\n",
    "                       #'auto_regr3','auto_regr4',\n",
    "                       #adding 3rd and 4th order AR terms increases performance slightly in\n",
    "                       #most clusters. However, it has the drawback of looking so far into\n",
    "                       #the past, that analysing the lockdown weeks would entail including a lot\n",
    "                       #of early weeks where infection data is still weird and unstable.\n",
    "                       #Also, having more AR terms than lagged exogenous factors feels slightly\n",
    "                       #weird, but there's definitely mathematical precedent for a (4,2)-ARX model,\n",
    "                       #and it's feasible that a phenomenon influences itself longer than it \n",
    "                       #is influenced by exogenous events. What's your opinion here?\n",
    "                       'population_density',\n",
    "                       'hospital_capacity',\n",
    "                       #'share_of_nursing_home_care', #this one to be decided based on predictive power\n",
    "                       #since it's predictive power is very low (like all other static predictors!)\n",
    "                       #this one's out for sure. Should we even bother with the other static predictors?\n",
    "                       'average_age', \n",
    "                       'average_household_size',\n",
    "                       'share_female',\n",
    "                       'log_intl_index',\n",
    "                       'Count_internal_workday_p_pop',\n",
    "                       'Count_internal_day_off_p_pop',\n",
    "                       'Count_incoming_workday_p_pop', \n",
    "                       'Count_incoming_day_off_p_pop',\n",
    "                       'incoming_infected_p_pop',\n",
    "                       'temperature_workday', # do we need all weather features or should we focus\n",
    "                       'humidity_workday',# on the ones most predictive (I'd say sunshine and temperature)?\n",
    "                       'precipitation_workday', #Also, what to make of the risk that these just serve as\n",
    "                       'sunshine_workday',#proxies for the week_no dummies in an earlier analysis, i.e.\n",
    "                       'velocity_workday',#it's not the actual weather at all, but just the time of year?\n",
    "                       'temperature_day_off',#And should we keep the workday/day off distinction for\n",
    "                       'humidity_day_off', #weather features?\n",
    "                       'precipitation_day_off',\n",
    "                       'sunshine_day_off',\n",
    "                       'velocity_day_off',\n",
    "                       #lagged vars add a little performance:\n",
    "                       'Count_internal_workday_p_pop_lag1',\n",
    "                       'Count_internal_day_off_p_pop_lag1',\n",
    "                       'Count_incoming_workday_p_pop_lag1',\n",
    "                       'Count_incoming_day_off_p_pop_lag1',\n",
    "                       'incoming_infected_p_pop_lag1',\n",
    "                       'temperature_workday_lag1',\n",
    "                       'humidity_workday_lag1',\n",
    "                       'precipitation_workday_lag1',\n",
    "                       'sunshine_workday_lag1',\n",
    "                       'velocity_workday_lag1',\n",
    "                       'temperature_day_off_lag1',\n",
    "                       'humidity_day_off_lag1',\n",
    "                       'precipitation_day_off_lag1',\n",
    "                       'sunshine_day_off_lag1',\n",
    "                       'velocity_day_off_lag1'\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0, N = 1085\n",
      "Overall, the cross-predictions accounted for 26.13% of the target variance.\n",
      "Cluster: 1, N = 2415\n",
      "Overall, the cross-predictions accounted for 16.06% of the target variance.\n",
      "Cluster: 2, N = 3710\n",
      "Overall, the cross-predictions accounted for 21.01% of the target variance.\n",
      "Cluster: 3, N = 2205\n",
      "Overall, the cross-predictions accounted for 16.51% of the target variance.\n",
      "Cluster: 4, N = 4585\n",
      "Overall, the cross-predictions accounted for 18.23% of the target variance.\n"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame()\n",
    "accuracies= []\n",
    "mod_coefs = []\n",
    "for clust in np.unique(all_data.cluster):\n",
    "    clust_df = all_data[all_data.cluster == clust].copy()\n",
    "    clust_N = clust_df.shape[0]\n",
    "    print('Cluster: {}, N = {}'.format(clust,clust_N))\n",
    "        \n",
    "    clust_output, accs, mod_attr = cross_predict(data = clust_df,\n",
    "                                                         X_cols = predictor_variables,\n",
    "                                                         y_col = 'target',\n",
    "                                                         id_col = 'districtId',\n",
    "                                                         how = 's',\n",
    "                                                         shuffle_mode='id',\n",
    "                                                         model = 'standard_linreg',\n",
    "                                                         mod_attrs_2_return = 'coef_',\n",
    "                                                         z_trim = 3,\n",
    "                                                         mod_params=None)\n",
    "\n",
    "    predictions = pd.concat([predictions, clust_output])\n",
    "    accuracies.append(accs)\n",
    "    mod_coefs.append(mod_attr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = ['r','g','b','y','c']\n",
    "# mean_dict = {}\n",
    "# for i,var in enumerate(predictor_variables):\n",
    "#     mean_dict[var] = []\n",
    "#     for clust in range(5):\n",
    "#         values = []\n",
    "#         for array_ in mod_coefs[clust]['coef_']:\n",
    "#             values.append(array_[i])\n",
    "#         mean_dict[var].append(np.mean(values))\n",
    "#         sns.distplot(values, \n",
    "#                      hist=False,\n",
    "#                      color = colors[clust])\n",
    "#     plt.title(var)\n",
    "# #     plt.xlim([-1,1])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in mean_dict.items():\n",
    "#     plt.barh(range(5),v)\n",
    "#     plt.title(k)\n",
    "#     plt.xlim([-.3,.3])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.43554856]\n",
      " [0.43554856 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "predictions['deviance'] = predictions['target_predicted'] - predictions['target']\n",
    "print(np.corrcoef(predictions['target_predicted'], predictions['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, predictions, left_index=True, right_index=True, suffixes=('','_del')).drop(columns=['districtId_del','target_del'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x14d061e22c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.jointplot(predictions.target, predictions.target_predicted,kind='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('all_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to check individual predictors:\n",
    "# for k in range(len(predictor_variables)):\n",
    "#     print(predictor_variables[k])\n",
    "#     predictions = pd.DataFrame()\n",
    "#     accuracies= []\n",
    "#     mod_coefs = []\n",
    "#     for clust in np.unique(all_data.cluster):\n",
    "#         clust_df = all_data[all_data.cluster == clust].copy()\n",
    "#         clust_N = clust_df.shape[0]\n",
    "#         print('Cluster: {}, N = {}'.format(clust,clust_N))\n",
    "\n",
    "#         clust_output, accs, mod_attr = cross_predict(data = clust_df,\n",
    "#                                                              X_cols = [predictor_variables[k]],\n",
    "#                                                              y_col = 'target',\n",
    "#                                                              id_col = 'districtId',\n",
    "#                                                              how = 's',\n",
    "#                                                              shuffle_mode='id',\n",
    "#                                                              model = 'standard_linreg',\n",
    "#                                                              mod_attrs_2_return = 'coef_',\n",
    "#                                                              z_trim = None,\n",
    "#                                                              mod_params=None)\n",
    "\n",
    "#         predictions = pd.concat([predictions, clust_output])\n",
    "#         accuracies.append(accs)\n",
    "#         mod_coefs.append(mod_attr)\n",
    "#     for c in range(5):\n",
    "#         print(np.mean([mod_coefs[c]['coef_'][i][0] for i in range(len(mod_coefs[c]['coef_']))]))\n",
    "#     print('\\n\\n ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
